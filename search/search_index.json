{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Stat 614.02 - Data Analysis Using Statistical Software Course Information Class time: TuTh 5:45 - 9:00pm Classroom: Hunter North 305 Instructor: Bora Ferlengez Best way to reach to me is shooting a message in our Telegram chat (stats/software related comments/questions) or send a private message (personal issues) on Telegram. But if you want to send an email: bora.ferlengez@gmail.com Office hours : TuTh 4:30 - 5:30pm (HE924), or after class, or by appointment. Textbook: R by Example, by Jim Albert and Maria Rizzo. 2012. Free download from the Hunter College library website. ISBN: 978-1-4614-1364-6. The data that is used in the book and the codes can be found in this GitHub repository created by Maria Rizzo. Also, in this repository , you can find solutions to some exercises. It is not an official repository, so I can vouch if the solutions are all correct. You can also find interesting education data in National Center for Education Statistics or in NYC Open Data / Education Reference Textbook: Introduction to Statistics and Data Analysis, Peck, Olsen and Devore, 5th edition. ISBN:978-1-305-11534-7.","title":"Syllabus"},{"location":"#welcome-to-stat-61402-data-analysis-using-statistical-software","text":"","title":"Welcome to Stat 614.02 - Data Analysis Using Statistical Software"},{"location":"#course-information","text":"Class time: TuTh 5:45 - 9:00pm Classroom: Hunter North 305 Instructor: Bora Ferlengez Best way to reach to me is shooting a message in our Telegram chat (stats/software related comments/questions) or send a private message (personal issues) on Telegram. But if you want to send an email: bora.ferlengez@gmail.com Office hours : TuTh 4:30 - 5:30pm (HE924), or after class, or by appointment. Textbook: R by Example, by Jim Albert and Maria Rizzo. 2012. Free download from the Hunter College library website. ISBN: 978-1-4614-1364-6. The data that is used in the book and the codes can be found in this GitHub repository created by Maria Rizzo. Also, in this repository , you can find solutions to some exercises. It is not an official repository, so I can vouch if the solutions are all correct. You can also find interesting education data in National Center for Education Statistics or in NYC Open Data / Education Reference Textbook: Introduction to Statistics and Data Analysis, Peck, Olsen and Devore, 5th edition. ISBN:978-1-305-11534-7.","title":"Course Information"},{"location":"chapter_1_code_snippets/","text":"Getting Started We have seen most of the material below before. The examples below will provide good practice. Having written that, there are quite a number of new commands/ideas, as well. Example 1 The purpose of this example is to get familiar how to create a vector using the combine c() function. Once you have a vector, arithmetic operations can be run using very simple syntax. You can make vectors algebraically interact with scalars (i.e. numbers) or with other vectors of same length (termwise). # create a vector and assign a variable. temps = c(51.9, 51.8, 51.9, 53) # subtract 32 from every item in the vector temps, and multiply the differences by 5/9, # that's the conversion formula from fahrenheit to celcius. (5/9) * (temps - 32) # create another vector. CT = c(48, 48.2, 48, 48.7) # when you subtract a vector from another, R runs the subtraction termwise. temps - CT .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 11.0555555555556 11 11.0555555555556 11.6666666666667 .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 3.9 3.59999999999999 3.9 4.3 Example 2 In this example, we practice indexing and slicing. By provoding the desired indices, we can grab the items at those indices of a vector. If we want to grab several items at once, the : operator comes very handy to generate a sequence of indices. The two new functions you may want to learn about would be the table() and barplot() functions. table() is used to create a frequency table. more on table() : https://www.statology.org/table-function-in-r/ more on barplot() : https://statisticsglobe.com/barplot-in-r # create two vectors from given data. winner = c(185, 182, 182, 188, 188, 188, 185, 185, 177, 182, 182, 193, 183, 179, 179, 175) opponent = c(175, 193, 185, 187, 188, 173, 180, 177, 183, 185, 180, 180, 182, 178, 178, 173) # you can see the lengths of the vectors in the environment pane of RStudio, # but you may want to compute with that length. # the `length()` function can be used to in those cases. length(winner) # `seq()` function is used to created a vector whose first item is 2008, last item 1948, # and the each item is 4 less than the previous item (i.e. the stepsize of the arithmetic sequence is -4) year = seq(from=2008, to=1948, by=-4) # grabbing the 4th item in the vector `winner`, # and updating that item with the value 189. winner[4] = 189 # grabbing the 5th item in the vector in `winner`, # and updating that item as 189. winner[5] = 189 # instead of the previous two commands, # we can grab the 4th and 5th items and update them with 189. winner[4:5] = 189 # the function `mean()` computes the mean (i.e. average) of a numerical vector. mean(winner) # the average of the `opponent` vector is computed. mean(opponent) # like in example 1, we can compute the difference of vectors (of same length), # that produces of a new vector whose items are the differences of the corresponding values in the given vectors. difference = winner - opponent # we create a data frame from the vectors `year`, `winner`, `opponent` and `difference` # those vectors will be columns of the data frame in the same order of the arguments of the function `data.frame()` data.frame(year, winner, opponent, difference) # the one below is a tricky line: # `winner > opponent` asks for every index of the vectors, if the item of `winner` vector is greater than # the correspondent item in the `opponent` vector. # the result of each of those comparisons is `TRUE` or `FALSE`. # hence `winner > opponent` creates a logical vector. # then that logical vector is assigned the variable name `taller.won` taller.won = winner > opponent # the `TRUE` and `FALSE`s are counted in the `taller.won` vector # the number of `TRUE`s is the the number of the elected president being taller than the other candidate. table(taller.won) # you can create the relative frequency (in percent) table by dividing each frequency by the total frequency # and multiplying by 100. table(taller.won) / 16 * 100 # finally a barplot is constructed. # one minor thing below: we want the values in the horizontal axis to increase as we move to right, # whereas our vectors constructed with election years in decreasing order. # 'rev(difference)' reverses the `difference` vector. # `barplot(rev(difference))` puts a bar at every index of the `rev(difference)` vector as tall as the value at that index. # `xlab` and `ylab` arguments of the `barplot()` function allow us name the x (horizontal) and y (vertical) axes. barplot(rev(difference), xlab=\"Election years 1948 to 2008\", ylab=\"Height difference in cm\") # note that the bars are not labed horizontally, # if you want horizontal labels, you can do the following: barplot(rev(difference), names.arg = rev(year), xlab=\"Election years 1948 to 2008\", ylab=\"Height difference in cm\") # depending on the size of the barplot, some horizontal labels can be skipped as R doesn't want the labels to overlap. # you can also easily draw a scatterplot for the pair of variables: `winner` and `opponent` # for every election, a point is generated whose x coordinate is the winner and y coordinate is the opponent. plot(winner,opponent) 16 183.4375 181.0625 A data.frame: 16 \u00d7 4 year winner opponent difference <dbl> <dbl> <dbl> <dbl> 2008 185 175 10 2004 182 193 -11 2000 182 185 -3 1996 189 187 2 1992 189 188 1 1988 188 173 15 1984 185 180 5 1980 185 177 8 1976 177 183 -6 1972 182 185 -3 1968 182 180 2 1964 193 180 13 1960 183 182 1 1956 179 178 1 1952 179 178 1 1948 175 173 2 taller.won FALSE TRUE 4 12 taller.won FALSE TRUE 25 75 Example 3 In this example, deaths of Prussian soldiers due to horsekicks are studied (I know, we may not be the target audience.) Towards the end of the example, our data is compared to Poisson distribution, which models the probability of a given number of events occurring in a fixed interval of time, like what's the probability that your name is called a fixed number of times during a lecture. To learn more about Poisson distributions, you can take a look at the wikipedia page https://en.wikipedia.org/wiki/Poisson_distribution or these notes for a somewhat intuitive description. # first, the data is stored. # `k` stores the number of deaths # `x` stores the frequencies of the values in `k`. k = c(0, 1, 2, 3, 4) x = c(109, 65, 22, 3, 1) # first, we draw a barplot # remember that the first argument denotes the heights of the bars. # and that the `names.arg` argument in the `barplot` function allows us to specify our horizontal labels. # barplot(x, names.arg = k, ylab=\"Frequencies\", xlab = 'Number of deaths', main = 'Frequency distribution') # to get the relative frequency barplot, we simply divide the frequencies by the total frequency. # and produce a new bar plot using those heights (the name `p` stands for percentage, i suppose.) p = x / sum(x) barplot(p, names.arg=k, ylab='Relative Frequencies', xlab = 'Number of deaths', main = 'Relative Freq Distr') # remember that the mean is a weighted sum of relative frequencies, as we discussed in class # the way the sample mean `r` is computed below is as follows: # `p * k` computes the product of each horizontal value with the height of the corresponding bar. # then those little products are added. that way the sample mean is calculated. r = sum(p * k) # you can think of the variance as the square of the average error of a horizontal value from the mean. # i'll talk more about the variance and standard deviation in the class. v = sum(x * (k - r)^2) / 199 You may find the rest of the example a bit confusing. What's done below is: - First, using our sample mean, the probabilities are modeled using a Poisson distribution. Why Poisson? \"How many deaths per year\" is the kind of thing Poisson distributions model well. Also, Poisson distributions' mean equals their variance, and our sample mean and sample variance are pretty close, which can be taken as further evidence for using Poisson distributions to model our data. # model the theoretical probabilities using a Poisson distribution whose mean equals to our sample mean. # f denotes the relative frequencies (i.e. probabilities). f = r^k * exp(- r) / factorial(k) f .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 0.5433508690745 0.331444030135445 0.101090429191311 0.0205550539355665 0.00313464572517389 We can generate the Poisson distribution above using a built-in function of R . f = dpois(k, r) f .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 0.5433508690745 0.331444030135445 0.101090429191311 0.0205550539355665 0.00313464572517389 Then we multiply those theoretical relative frequencies generated by the Poisson distribution by 200 (our total frequency) and round the results down to get integer (theoretical) frequencies. floor(200*f) .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 108 66 20 4 0 Remember our actual frequencies: x .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 109 65 22 3 1 Note that our frequencies are pretty close to the theoretical frequencies of Poisson distribution with a mean that equals our sample mean, and when the sample size is 200 like ours. Lastly, we combine the vectors k (labels), p (our actual relative frequencies) and f (the theoretical relative frequencies) to see how good our sample data can be modeled by a Poission distribution. This is similar to the comparison of theoretical and our frequencies we did above, except at the relative frequency level. cbind(k, p, f) A matrix: 5 \u00d7 3 of type dbl k p f 0 0.545 0.543350869 1 0.325 0.331444030 2 0.110 0.101090429 3 0.015 0.020555054 4 0.005 0.003134646 Example 4 The purpose of this example is to get familiar with using code snippets in R . One important takeaway from this example is to learn how to use R scripts in RStudio . If you choose File > New File > R Script in RStudio 's top menu and paste the following code snippet (a code snippet is a programming term that refers to a small portion of re-usable source code) into the pane on upper left corner and click the Source key, RStudio will run all the lines you pasted. You can save those code snippets in your hard-drive for future use. A good practice is to have a particular directory on your hard drive for those scripts, you name those files in a descriptive way when saving, you use the extension .R . Example: horsekicks.r One remark: When you want a variable to be displayed on console, you'll need to specify that you want it to be printed by using the print() function in your script. If you want to read more about the print() function: https://riptutorial.com/r/example/1221/printing-and-displaying-strings # Prussian horsekick data # rpois() generates random values according to the Poisson distribution and creates an artificially simulated sample. # we take the mean = .61 like in example 3, and sample size = 200. y = rpois(200, lambda=.61) # create a table of sample frequencies. kicks = table(y) # create a table of sample relative frequencies. sample = kicks / 200 # display the table of sample relative frequencies. print(sample) # create a theoretical relative distribution according to Poisson distribution with mean = 0.61 theoretical = dpois(0:3, lambda=.61) # display the theoretical relative frequencies. print(theoretical) # combine the theoretical and sample relative frequencies in a table: cbind(theoretical, Sample) # print the table print(cbind(theoretical, Sample)) y 0 1 2 3 4 0.525 0.320 0.130 0.020 0.005 [1] 0.54335087 0.33144403 0.10109043 0.02055505 A matrix: 4 \u00d7 2 of type dbl theoretical Sample 0 0.54335087 0.515 1 0.33144403 0.330 2 0.10109043 0.135 3 0.02055505 0.020 theoretical Sample 0 0.54335087 0.515 1 0.33144403 0.330 2 0.10109043 0.135 3 0.02055505 0.020 One interesting remark is that the actual data in example 3 fits the theoretical Poisson distribution better than the simulated data in example 4. The R Help System # to ask about a function/keyword. ?barplot # alternative way to ask about a function/keyword. help(barplot) # searches through the help documentation for the keyword and finds all the documentations that contain the keyword. ??dpois # searches through the help documentation for the keyword and finds all the documentations that contain the keyword. help.search(\"dpois\") # to get an example of how to use a function. example(mean) mean> x <- c(0:10, 50) mean> xm <- mean(x) mean> c(xm, mean(x, trim = 0.10)) [1] 8.75 5.50 # usually, there is an example at the end of the help file, as well. ?mean Functions Example 5 In this example, defining and using a function are illustrated. We have discussed what functions are and how to define a function (the syntax of defining functions) last week. # defining the function whose name will be var.n # and which computes the sample variance of a vector of numbers. var.n = function(x){ v = var(x) n = NROW(x) v * (n - 1) / n } # using the function. temps = c(51.9, 51.8, 51.9, 53) var(temps) var.n(temps) 0.323333333333334 0.242500000000001 Example 6 Omitted, because we don't need to discuss integration (a calculus operation) at this point. Example 7 The ( R ) function curve() which draws the graph of the (mathematical & R ) function in its argument. # remark: if the definition is one line { } are not needed. f = function(x, a=1, b=1) x^(a-1) * (1-x)^(b-1) # graphing a function from real numbers to real numbers. # remark: the function to be graphed should always be defined in terms of x. curve(f(x,2,5), from=0, to=1) # if you like, you can label the x and y axes. curve(f(x,2,5), from=0, to=1, xlab=\"inputs\", ylab='outputs') Vectors and Matrices Example 8 Below, we create a matrix that represents the class mobility of generations. The entry $P_{ij}$ (namely, the entry in row $i$, column $j$) represents the probability the child being in the class represented by column $j$, if the parents are in row $i$. # creating a matrix from given data. probs = c(.45, .05, .01, .48, .70, .50, .07, .25, .49) P = matrix(probs, nrow=3, ncol=3) P # naming rows and columns. rownames(P) <- c(\"lower_old\", \"middle_old\", \"upper_old\") colnames(P) <- c(\"lower_new\", \"middle_new\", \"upper_new\") P # how to obtain row sums. rowSums(P) # how to obtain column sums. colSums(P) # grabbing the entry in row 1, column 3 P[1, 3] # grabbing the whole row 1 P[1, ] # grabbing the whole column 2 P[ ,2] A matrix: 3 \u00d7 3 of type dbl 0.45 0.48 0.07 0.05 0.70 0.25 0.01 0.50 0.49 A matrix: 3 \u00d7 3 of type dbl lower_new middle_new upper_new lower_old 0.45 0.48 0.07 middle_old 0.05 0.70 0.25 upper_old 0.01 0.50 0.49 .dl-inline {width: auto; margin:0; padding: 0} .dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block} .dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex} .dl-inline>dt:not(:first-of-type) {padding-left: .5ex} lower_old 1 middle_old 1 upper_old 1 .dl-inline {width: auto; margin:0; padding: 0} .dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block} .dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex} .dl-inline>dt:not(:first-of-type) {padding-left: .5ex} lower_new 0.51 middle_new 1.68 upper_new 0.81 0.07 .dl-inline {width: auto; margin:0; padding: 0} .dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block} .dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex} .dl-inline>dt:not(:first-of-type) {padding-left: .5ex} lower_new 0.45 middle_new 0.48 upper_new 0.07 .dl-inline {width: auto; margin:0; padding: 0} .dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block} .dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex} .dl-inline>dt:not(:first-of-type) {padding-left: .5ex} lower_old 0.48 middle_old 0.7 upper_old 0.5 # it's probably easier to define a matrix using the following command, # instead of typing two lines as above example. # `byrow=TRUE` argument guarantees that the numbers are placed in rows # and once sufficiently many numbers are placed, # the next number is placed as the first entry in the row below. Q = matrix(c( 0.45, 0.48, 0.07, 0.05, 0.70, 0.25, 0.01, 0.50, 0.49), nrow=3, ncol=3, byrow=TRUE) Q A matrix: 3 \u00d7 3 of type dbl 0.45 0.48 0.07 0.05 0.70 0.25 0.01 0.50 0.49 It's easy to do matrix operations in R . We probably won't need those operations, in case we do, I can go over basic matrix operations. But, here's the list anyways: (Ignore the operations that doesn't make sense to you.) # elementwise multiplication P * Q # exponentiating elementwise P^2 # matrix multiplication P %*% Q # inverse matrix solve(P) A matrix: 3 \u00d7 3 of type dbl lower_new middle_new upper_new lower_old 0.2025 0.2304 0.0049 middle_old 0.0025 0.4900 0.0625 upper_old 0.0001 0.2500 0.2401 A matrix: 3 \u00d7 3 of type dbl lower_new middle_new upper_new lower_old 0.2025 0.2304 0.0049 middle_old 0.0025 0.4900 0.0625 upper_old 0.0001 0.2500 0.2401 A matrix: 3 \u00d7 3 of type dbl lower_old 0.2272 0.5870 0.1858 middle_old 0.0600 0.6390 0.3010 upper_old 0.0344 0.5998 0.3658 A matrix: 3 \u00d7 3 of type dbl lower_old middle_old upper_old lower_new 2.4549550 -2.254505 0.7995495 middle_new -0.2477477 2.475225 -1.2274775 upper_new 0.2027027 -2.479730 3.2770270 Data Frames Data frames are different from the matrices. The data type of entries of a matrix are all the same. You can think of a matrix as a 2D vector. On the other hand, a data frame consists of vertical vectors. Namely, each item in the same column has the same data type, but items in different columns don't have to have the same data type. It's like stacking vertical vectors horizontally. Example 9 In this example, the purpose is to learn different ways of displaying a data frame and getting summary information of a data frame. # displaying all data USArrests A data.frame: 50 \u00d7 4 Murder Assault UrbanPop Rape <dbl> <int> <int> <dbl> Alabama 13.2 236 58 21.2 Alaska 10.0 263 48 44.5 Arizona 8.1 294 80 31.0 Arkansas 8.8 190 50 19.5 California 9.0 276 91 40.6 Colorado 7.9 204 78 38.7 Connecticut 3.3 110 77 11.1 Delaware 5.9 238 72 15.8 Florida 15.4 335 80 31.9 Georgia 17.4 211 60 25.8 Hawaii 5.3 46 83 20.2 Idaho 2.6 120 54 14.2 Illinois 10.4 249 83 24.0 Indiana 7.2 113 65 21.0 Iowa 2.2 56 57 11.3 Kansas 6.0 115 66 18.0 Kentucky 9.7 109 52 16.3 Louisiana 15.4 249 66 22.2 Maine 2.1 83 51 7.8 Maryland 11.3 300 67 27.8 Massachusetts 4.4 149 85 16.3 Michigan 12.1 255 74 35.1 Minnesota 2.7 72 66 14.9 Mississippi 16.1 259 44 17.1 Missouri 9.0 178 70 28.2 Montana 6.0 109 53 16.4 Nebraska 4.3 102 62 16.5 Nevada 12.2 252 81 46.0 New Hampshire 2.1 57 56 9.5 New Jersey 7.4 159 89 18.8 New Mexico 11.4 285 70 32.1 New York 11.1 254 86 26.1 North Carolina 13.0 337 45 16.1 North Dakota 0.8 45 44 7.3 Ohio 7.3 120 75 21.4 Oklahoma 6.6 151 68 20.0 Oregon 4.9 159 67 29.3 Pennsylvania 6.3 106 72 14.9 Rhode Island 3.4 174 87 8.3 South Carolina 14.4 279 48 22.5 South Dakota 3.8 86 45 12.8 Tennessee 13.2 188 59 26.9 Texas 12.7 201 80 25.5 Utah 3.2 120 80 22.9 Vermont 2.2 48 32 11.2 Virginia 8.5 156 63 20.7 Washington 4.0 145 73 26.2 West Virginia 5.7 81 39 9.3 Wisconsin 2.6 53 66 10.8 Wyoming 6.8 161 60 15.6 # sometimes (and by sometimes i mean almost always) # there is too much data to display, # in those cases, displaying few top or bottom rows # is a good idea to see what kind of data we have. head(USArrests) # if you want to see more rows from top, you can specify that. head(USArrests, 14) # similarly, you can display few bottom rows, as well, # using the `tail()` function. tail(USArrests) A data.frame: 6 \u00d7 4 Murder Assault UrbanPop Rape <dbl> <int> <int> <dbl> Alabama 13.2 236 58 21.2 Alaska 10.0 263 48 44.5 Arizona 8.1 294 80 31.0 Arkansas 8.8 190 50 19.5 California 9.0 276 91 40.6 Colorado 7.9 204 78 38.7 A data.frame: 14 \u00d7 4 Murder Assault UrbanPop Rape <dbl> <int> <int> <dbl> Alabama 13.2 236 58 21.2 Alaska 10.0 263 48 44.5 Arizona 8.1 294 80 31.0 Arkansas 8.8 190 50 19.5 California 9.0 276 91 40.6 Colorado 7.9 204 78 38.7 Connecticut 3.3 110 77 11.1 Delaware 5.9 238 72 15.8 Florida 15.4 335 80 31.9 Georgia 17.4 211 60 25.8 Hawaii 5.3 46 83 20.2 Idaho 2.6 120 54 14.2 Illinois 10.4 249 83 24.0 Indiana 7.2 113 65 21.0 A data.frame: 6 \u00d7 4 Murder Assault UrbanPop Rape <dbl> <int> <int> <dbl> Vermont 2.2 48 32 11.2 Virginia 8.5 156 63 20.7 Washington 4.0 145 73 26.2 West Virginia 5.7 81 39 9.3 Wisconsin 2.6 53 66 10.8 Wyoming 6.8 161 60 15.6 Since we're using RStudio , we can see the information below easily. In case, we want to display those information in the console or compute with them, this is how it's done: ## size/summary of data # str short for structure # if you prefer to work with matrices: arrests = as.matrix(USArrests) str(arrests) num [1:50, 1:4] 13.2 10 8.1 8.8 9 7.9 3.3 5.9 15.4 17.4 ... - attr(*, \"dimnames\")=List of 2 ..$ : chr [1:50] \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ... ..$ : chr [1:4] \"Murder\" \"Assault\" \"UrbanPop\" \"Rape\" Example 10 Summary statistics and basic visualization. summary(USArrests) Murder Assault UrbanPop Rape Min. : 0.800 Min. : 45.0 Min. :32.00 Min. : 7.30 1st Qu.: 4.075 1st Qu.:109.0 1st Qu.:54.50 1st Qu.:15.07 Median : 7.250 Median :159.0 Median :66.00 Median :20.10 Mean : 7.788 Mean :170.8 Mean :65.54 Mean :21.23 3rd Qu.:11.250 3rd Qu.:249.0 3rd Qu.:77.75 3rd Qu.:26.18 Max. :17.400 Max. :337.0 Max. :91.00 Max. :46.00 # extracting data USArrests[\"California\", \"Murder\"] USArrests[\"California\", ] USArrests[, \"Murder\"] USArrests$Murder 9 A data.frame: 1 \u00d7 4 Murder Assault UrbanPop Rape <dbl> <int> <int> <dbl> California 9 276 91 40.6 .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 13.2 10 8.1 8.8 9 7.9 3.3 5.9 15.4 17.4 5.3 2.6 10.4 7.2 2.2 6 9.7 15.4 2.1 11.3 4.4 12.1 2.7 16.1 9 6 4.3 12.2 2.1 7.4 11.4 11.1 13 0.8 7.3 6.6 4.9 6.3 3.4 14.4 3.8 13.2 12.7 3.2 2.2 8.5 4 5.7 2.6 6.8 .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 13.2 10 8.1 8.8 9 7.9 3.3 5.9 15.4 17.4 5.3 2.6 10.4 7.2 2.2 6 9.7 15.4 2.1 11.3 4.4 12.1 2.7 16.1 9 6 4.3 12.2 2.1 7.4 11.4 11.1 13 0.8 7.3 6.6 4.9 6.3 3.4 14.4 3.8 13.2 12.7 3.2 2.2 8.5 4 5.7 2.6 6.8 # to create a frequency histogram hist(USArrests[, \"Murder\"]) hist(USArrests$Murder) # to create a probability (relative frequency) histogram library(MASS) truehist(USArrests$Murder) hist(USArrests$Murder, prob=TRUE, breaks=\"scott\") USArrests$Murder .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 13.2 10 8.1 8.8 9 7.9 3.3 5.9 15.4 17.4 5.3 2.6 10.4 7.2 2.2 6 9.7 15.4 2.1 11.3 4.4 12.1 2.7 16.1 9 6 4.3 12.2 2.1 7.4 11.4 11.1 13 0.8 7.3 6.6 4.9 6.3 3.4 14.4 3.8 13.2 12.7 3.2 2.2 8.5 4 5.7 2.6 6.8 Attaching a data frame # to save time, sometimes we attach a data frame so that we can use the # row/column names directly without the need of slicing or $ sign. attach(USArrests) # then we can call Murder instead of USArrests$Murder murder.pct = 100 * Murder / (Murder + Assault + Rape) # to detach detach(USArrests) Scatterplots and correlations attach(USArrests) plot(UrbanPop, Murder) pairs(USArrests) cor(UrbanPop, Murder) 0.0695726217359934 cor(USArrests) A matrix: 4 \u00d7 4 of type dbl Murder Assault UrbanPop Rape Murder 1.00000000 0.8018733 0.06957262 0.5635788 Assault 0.80187331 1.0000000 0.25887170 0.6652412 UrbanPop 0.06957262 0.2588717 1.00000000 0.4113412 Rape 0.56357883 0.6652412 0.41134124 1.0000000 Importing Data You can following tutorial for more detail: http://www.r-tutor.com/r-introduction/data-frame/data-import However, a quick way to import a csv file as a data frame in RStudio is as follows: 1. Download and know where the csv file is. 2. Use the read.csv() function, and put file.choose() as an argument. RStudio will open a window that will let you find the file you want to import. If your csv file has headers, then please also put the argument header=T into the read.csv() function. data <- read.csv(file.choose(), header=T) The most common error is you obtain only one column. The reason is usually that you didn't choose the correct delimiter (what character seperates the values in a row). By looking at the data frame, you can find out what delimiter was to be used. The common choices are: - ',' (comma) - ';' (semicolon) - '\\t' (tab) # to import a csv file with header, where the values in a row are seperated by tab: data = read.csv(file.choose(), sep='\\t', header = T)","title":"01 - Chapter 1"},{"location":"chapter_1_code_snippets/#getting-started","text":"We have seen most of the material below before. The examples below will provide good practice. Having written that, there are quite a number of new commands/ideas, as well.","title":"Getting Started"},{"location":"chapter_1_code_snippets/#example-1","text":"The purpose of this example is to get familiar how to create a vector using the combine c() function. Once you have a vector, arithmetic operations can be run using very simple syntax. You can make vectors algebraically interact with scalars (i.e. numbers) or with other vectors of same length (termwise). # create a vector and assign a variable. temps = c(51.9, 51.8, 51.9, 53) # subtract 32 from every item in the vector temps, and multiply the differences by 5/9, # that's the conversion formula from fahrenheit to celcius. (5/9) * (temps - 32) # create another vector. CT = c(48, 48.2, 48, 48.7) # when you subtract a vector from another, R runs the subtraction termwise. temps - CT .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 11.0555555555556 11 11.0555555555556 11.6666666666667 .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 3.9 3.59999999999999 3.9 4.3","title":"Example 1"},{"location":"chapter_1_code_snippets/#example-2","text":"In this example, we practice indexing and slicing. By provoding the desired indices, we can grab the items at those indices of a vector. If we want to grab several items at once, the : operator comes very handy to generate a sequence of indices. The two new functions you may want to learn about would be the table() and barplot() functions. table() is used to create a frequency table. more on table() : https://www.statology.org/table-function-in-r/ more on barplot() : https://statisticsglobe.com/barplot-in-r # create two vectors from given data. winner = c(185, 182, 182, 188, 188, 188, 185, 185, 177, 182, 182, 193, 183, 179, 179, 175) opponent = c(175, 193, 185, 187, 188, 173, 180, 177, 183, 185, 180, 180, 182, 178, 178, 173) # you can see the lengths of the vectors in the environment pane of RStudio, # but you may want to compute with that length. # the `length()` function can be used to in those cases. length(winner) # `seq()` function is used to created a vector whose first item is 2008, last item 1948, # and the each item is 4 less than the previous item (i.e. the stepsize of the arithmetic sequence is -4) year = seq(from=2008, to=1948, by=-4) # grabbing the 4th item in the vector `winner`, # and updating that item with the value 189. winner[4] = 189 # grabbing the 5th item in the vector in `winner`, # and updating that item as 189. winner[5] = 189 # instead of the previous two commands, # we can grab the 4th and 5th items and update them with 189. winner[4:5] = 189 # the function `mean()` computes the mean (i.e. average) of a numerical vector. mean(winner) # the average of the `opponent` vector is computed. mean(opponent) # like in example 1, we can compute the difference of vectors (of same length), # that produces of a new vector whose items are the differences of the corresponding values in the given vectors. difference = winner - opponent # we create a data frame from the vectors `year`, `winner`, `opponent` and `difference` # those vectors will be columns of the data frame in the same order of the arguments of the function `data.frame()` data.frame(year, winner, opponent, difference) # the one below is a tricky line: # `winner > opponent` asks for every index of the vectors, if the item of `winner` vector is greater than # the correspondent item in the `opponent` vector. # the result of each of those comparisons is `TRUE` or `FALSE`. # hence `winner > opponent` creates a logical vector. # then that logical vector is assigned the variable name `taller.won` taller.won = winner > opponent # the `TRUE` and `FALSE`s are counted in the `taller.won` vector # the number of `TRUE`s is the the number of the elected president being taller than the other candidate. table(taller.won) # you can create the relative frequency (in percent) table by dividing each frequency by the total frequency # and multiplying by 100. table(taller.won) / 16 * 100 # finally a barplot is constructed. # one minor thing below: we want the values in the horizontal axis to increase as we move to right, # whereas our vectors constructed with election years in decreasing order. # 'rev(difference)' reverses the `difference` vector. # `barplot(rev(difference))` puts a bar at every index of the `rev(difference)` vector as tall as the value at that index. # `xlab` and `ylab` arguments of the `barplot()` function allow us name the x (horizontal) and y (vertical) axes. barplot(rev(difference), xlab=\"Election years 1948 to 2008\", ylab=\"Height difference in cm\") # note that the bars are not labed horizontally, # if you want horizontal labels, you can do the following: barplot(rev(difference), names.arg = rev(year), xlab=\"Election years 1948 to 2008\", ylab=\"Height difference in cm\") # depending on the size of the barplot, some horizontal labels can be skipped as R doesn't want the labels to overlap. # you can also easily draw a scatterplot for the pair of variables: `winner` and `opponent` # for every election, a point is generated whose x coordinate is the winner and y coordinate is the opponent. plot(winner,opponent) 16 183.4375 181.0625 A data.frame: 16 \u00d7 4 year winner opponent difference <dbl> <dbl> <dbl> <dbl> 2008 185 175 10 2004 182 193 -11 2000 182 185 -3 1996 189 187 2 1992 189 188 1 1988 188 173 15 1984 185 180 5 1980 185 177 8 1976 177 183 -6 1972 182 185 -3 1968 182 180 2 1964 193 180 13 1960 183 182 1 1956 179 178 1 1952 179 178 1 1948 175 173 2 taller.won FALSE TRUE 4 12 taller.won FALSE TRUE 25 75","title":"Example 2"},{"location":"chapter_1_code_snippets/#example-3","text":"In this example, deaths of Prussian soldiers due to horsekicks are studied (I know, we may not be the target audience.) Towards the end of the example, our data is compared to Poisson distribution, which models the probability of a given number of events occurring in a fixed interval of time, like what's the probability that your name is called a fixed number of times during a lecture. To learn more about Poisson distributions, you can take a look at the wikipedia page https://en.wikipedia.org/wiki/Poisson_distribution or these notes for a somewhat intuitive description. # first, the data is stored. # `k` stores the number of deaths # `x` stores the frequencies of the values in `k`. k = c(0, 1, 2, 3, 4) x = c(109, 65, 22, 3, 1) # first, we draw a barplot # remember that the first argument denotes the heights of the bars. # and that the `names.arg` argument in the `barplot` function allows us to specify our horizontal labels. # barplot(x, names.arg = k, ylab=\"Frequencies\", xlab = 'Number of deaths', main = 'Frequency distribution') # to get the relative frequency barplot, we simply divide the frequencies by the total frequency. # and produce a new bar plot using those heights (the name `p` stands for percentage, i suppose.) p = x / sum(x) barplot(p, names.arg=k, ylab='Relative Frequencies', xlab = 'Number of deaths', main = 'Relative Freq Distr') # remember that the mean is a weighted sum of relative frequencies, as we discussed in class # the way the sample mean `r` is computed below is as follows: # `p * k` computes the product of each horizontal value with the height of the corresponding bar. # then those little products are added. that way the sample mean is calculated. r = sum(p * k) # you can think of the variance as the square of the average error of a horizontal value from the mean. # i'll talk more about the variance and standard deviation in the class. v = sum(x * (k - r)^2) / 199 You may find the rest of the example a bit confusing. What's done below is: - First, using our sample mean, the probabilities are modeled using a Poisson distribution. Why Poisson? \"How many deaths per year\" is the kind of thing Poisson distributions model well. Also, Poisson distributions' mean equals their variance, and our sample mean and sample variance are pretty close, which can be taken as further evidence for using Poisson distributions to model our data. # model the theoretical probabilities using a Poisson distribution whose mean equals to our sample mean. # f denotes the relative frequencies (i.e. probabilities). f = r^k * exp(- r) / factorial(k) f .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 0.5433508690745 0.331444030135445 0.101090429191311 0.0205550539355665 0.00313464572517389 We can generate the Poisson distribution above using a built-in function of R . f = dpois(k, r) f .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 0.5433508690745 0.331444030135445 0.101090429191311 0.0205550539355665 0.00313464572517389 Then we multiply those theoretical relative frequencies generated by the Poisson distribution by 200 (our total frequency) and round the results down to get integer (theoretical) frequencies. floor(200*f) .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 108 66 20 4 0 Remember our actual frequencies: x .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 109 65 22 3 1 Note that our frequencies are pretty close to the theoretical frequencies of Poisson distribution with a mean that equals our sample mean, and when the sample size is 200 like ours. Lastly, we combine the vectors k (labels), p (our actual relative frequencies) and f (the theoretical relative frequencies) to see how good our sample data can be modeled by a Poission distribution. This is similar to the comparison of theoretical and our frequencies we did above, except at the relative frequency level. cbind(k, p, f) A matrix: 5 \u00d7 3 of type dbl k p f 0 0.545 0.543350869 1 0.325 0.331444030 2 0.110 0.101090429 3 0.015 0.020555054 4 0.005 0.003134646","title":"Example 3"},{"location":"chapter_1_code_snippets/#example-4","text":"The purpose of this example is to get familiar with using code snippets in R . One important takeaway from this example is to learn how to use R scripts in RStudio . If you choose File > New File > R Script in RStudio 's top menu and paste the following code snippet (a code snippet is a programming term that refers to a small portion of re-usable source code) into the pane on upper left corner and click the Source key, RStudio will run all the lines you pasted. You can save those code snippets in your hard-drive for future use. A good practice is to have a particular directory on your hard drive for those scripts, you name those files in a descriptive way when saving, you use the extension .R . Example: horsekicks.r One remark: When you want a variable to be displayed on console, you'll need to specify that you want it to be printed by using the print() function in your script. If you want to read more about the print() function: https://riptutorial.com/r/example/1221/printing-and-displaying-strings # Prussian horsekick data # rpois() generates random values according to the Poisson distribution and creates an artificially simulated sample. # we take the mean = .61 like in example 3, and sample size = 200. y = rpois(200, lambda=.61) # create a table of sample frequencies. kicks = table(y) # create a table of sample relative frequencies. sample = kicks / 200 # display the table of sample relative frequencies. print(sample) # create a theoretical relative distribution according to Poisson distribution with mean = 0.61 theoretical = dpois(0:3, lambda=.61) # display the theoretical relative frequencies. print(theoretical) # combine the theoretical and sample relative frequencies in a table: cbind(theoretical, Sample) # print the table print(cbind(theoretical, Sample)) y 0 1 2 3 4 0.525 0.320 0.130 0.020 0.005 [1] 0.54335087 0.33144403 0.10109043 0.02055505 A matrix: 4 \u00d7 2 of type dbl theoretical Sample 0 0.54335087 0.515 1 0.33144403 0.330 2 0.10109043 0.135 3 0.02055505 0.020 theoretical Sample 0 0.54335087 0.515 1 0.33144403 0.330 2 0.10109043 0.135 3 0.02055505 0.020 One interesting remark is that the actual data in example 3 fits the theoretical Poisson distribution better than the simulated data in example 4.","title":"Example 4"},{"location":"chapter_1_code_snippets/#the-r-help-system","text":"# to ask about a function/keyword. ?barplot # alternative way to ask about a function/keyword. help(barplot) # searches through the help documentation for the keyword and finds all the documentations that contain the keyword. ??dpois # searches through the help documentation for the keyword and finds all the documentations that contain the keyword. help.search(\"dpois\") # to get an example of how to use a function. example(mean) mean> x <- c(0:10, 50) mean> xm <- mean(x) mean> c(xm, mean(x, trim = 0.10)) [1] 8.75 5.50 # usually, there is an example at the end of the help file, as well. ?mean","title":"The R Help System"},{"location":"chapter_1_code_snippets/#functions","text":"","title":"Functions"},{"location":"chapter_1_code_snippets/#example-5","text":"In this example, defining and using a function are illustrated. We have discussed what functions are and how to define a function (the syntax of defining functions) last week. # defining the function whose name will be var.n # and which computes the sample variance of a vector of numbers. var.n = function(x){ v = var(x) n = NROW(x) v * (n - 1) / n } # using the function. temps = c(51.9, 51.8, 51.9, 53) var(temps) var.n(temps) 0.323333333333334 0.242500000000001","title":"Example 5"},{"location":"chapter_1_code_snippets/#example-6","text":"Omitted, because we don't need to discuss integration (a calculus operation) at this point.","title":"Example 6"},{"location":"chapter_1_code_snippets/#example-7","text":"The ( R ) function curve() which draws the graph of the (mathematical & R ) function in its argument. # remark: if the definition is one line { } are not needed. f = function(x, a=1, b=1) x^(a-1) * (1-x)^(b-1) # graphing a function from real numbers to real numbers. # remark: the function to be graphed should always be defined in terms of x. curve(f(x,2,5), from=0, to=1) # if you like, you can label the x and y axes. curve(f(x,2,5), from=0, to=1, xlab=\"inputs\", ylab='outputs')","title":"Example 7"},{"location":"chapter_1_code_snippets/#vectors-and-matrices","text":"","title":"Vectors and Matrices"},{"location":"chapter_1_code_snippets/#example-8","text":"Below, we create a matrix that represents the class mobility of generations. The entry $P_{ij}$ (namely, the entry in row $i$, column $j$) represents the probability the child being in the class represented by column $j$, if the parents are in row $i$. # creating a matrix from given data. probs = c(.45, .05, .01, .48, .70, .50, .07, .25, .49) P = matrix(probs, nrow=3, ncol=3) P # naming rows and columns. rownames(P) <- c(\"lower_old\", \"middle_old\", \"upper_old\") colnames(P) <- c(\"lower_new\", \"middle_new\", \"upper_new\") P # how to obtain row sums. rowSums(P) # how to obtain column sums. colSums(P) # grabbing the entry in row 1, column 3 P[1, 3] # grabbing the whole row 1 P[1, ] # grabbing the whole column 2 P[ ,2] A matrix: 3 \u00d7 3 of type dbl 0.45 0.48 0.07 0.05 0.70 0.25 0.01 0.50 0.49 A matrix: 3 \u00d7 3 of type dbl lower_new middle_new upper_new lower_old 0.45 0.48 0.07 middle_old 0.05 0.70 0.25 upper_old 0.01 0.50 0.49 .dl-inline {width: auto; margin:0; padding: 0} .dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block} .dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex} .dl-inline>dt:not(:first-of-type) {padding-left: .5ex} lower_old 1 middle_old 1 upper_old 1 .dl-inline {width: auto; margin:0; padding: 0} .dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block} .dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex} .dl-inline>dt:not(:first-of-type) {padding-left: .5ex} lower_new 0.51 middle_new 1.68 upper_new 0.81 0.07 .dl-inline {width: auto; margin:0; padding: 0} .dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block} .dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex} .dl-inline>dt:not(:first-of-type) {padding-left: .5ex} lower_new 0.45 middle_new 0.48 upper_new 0.07 .dl-inline {width: auto; margin:0; padding: 0} .dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block} .dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex} .dl-inline>dt:not(:first-of-type) {padding-left: .5ex} lower_old 0.48 middle_old 0.7 upper_old 0.5 # it's probably easier to define a matrix using the following command, # instead of typing two lines as above example. # `byrow=TRUE` argument guarantees that the numbers are placed in rows # and once sufficiently many numbers are placed, # the next number is placed as the first entry in the row below. Q = matrix(c( 0.45, 0.48, 0.07, 0.05, 0.70, 0.25, 0.01, 0.50, 0.49), nrow=3, ncol=3, byrow=TRUE) Q A matrix: 3 \u00d7 3 of type dbl 0.45 0.48 0.07 0.05 0.70 0.25 0.01 0.50 0.49 It's easy to do matrix operations in R . We probably won't need those operations, in case we do, I can go over basic matrix operations. But, here's the list anyways: (Ignore the operations that doesn't make sense to you.) # elementwise multiplication P * Q # exponentiating elementwise P^2 # matrix multiplication P %*% Q # inverse matrix solve(P) A matrix: 3 \u00d7 3 of type dbl lower_new middle_new upper_new lower_old 0.2025 0.2304 0.0049 middle_old 0.0025 0.4900 0.0625 upper_old 0.0001 0.2500 0.2401 A matrix: 3 \u00d7 3 of type dbl lower_new middle_new upper_new lower_old 0.2025 0.2304 0.0049 middle_old 0.0025 0.4900 0.0625 upper_old 0.0001 0.2500 0.2401 A matrix: 3 \u00d7 3 of type dbl lower_old 0.2272 0.5870 0.1858 middle_old 0.0600 0.6390 0.3010 upper_old 0.0344 0.5998 0.3658 A matrix: 3 \u00d7 3 of type dbl lower_old middle_old upper_old lower_new 2.4549550 -2.254505 0.7995495 middle_new -0.2477477 2.475225 -1.2274775 upper_new 0.2027027 -2.479730 3.2770270","title":"Example 8"},{"location":"chapter_1_code_snippets/#data-frames","text":"Data frames are different from the matrices. The data type of entries of a matrix are all the same. You can think of a matrix as a 2D vector. On the other hand, a data frame consists of vertical vectors. Namely, each item in the same column has the same data type, but items in different columns don't have to have the same data type. It's like stacking vertical vectors horizontally.","title":"Data Frames"},{"location":"chapter_1_code_snippets/#example-9","text":"In this example, the purpose is to learn different ways of displaying a data frame and getting summary information of a data frame. # displaying all data USArrests A data.frame: 50 \u00d7 4 Murder Assault UrbanPop Rape <dbl> <int> <int> <dbl> Alabama 13.2 236 58 21.2 Alaska 10.0 263 48 44.5 Arizona 8.1 294 80 31.0 Arkansas 8.8 190 50 19.5 California 9.0 276 91 40.6 Colorado 7.9 204 78 38.7 Connecticut 3.3 110 77 11.1 Delaware 5.9 238 72 15.8 Florida 15.4 335 80 31.9 Georgia 17.4 211 60 25.8 Hawaii 5.3 46 83 20.2 Idaho 2.6 120 54 14.2 Illinois 10.4 249 83 24.0 Indiana 7.2 113 65 21.0 Iowa 2.2 56 57 11.3 Kansas 6.0 115 66 18.0 Kentucky 9.7 109 52 16.3 Louisiana 15.4 249 66 22.2 Maine 2.1 83 51 7.8 Maryland 11.3 300 67 27.8 Massachusetts 4.4 149 85 16.3 Michigan 12.1 255 74 35.1 Minnesota 2.7 72 66 14.9 Mississippi 16.1 259 44 17.1 Missouri 9.0 178 70 28.2 Montana 6.0 109 53 16.4 Nebraska 4.3 102 62 16.5 Nevada 12.2 252 81 46.0 New Hampshire 2.1 57 56 9.5 New Jersey 7.4 159 89 18.8 New Mexico 11.4 285 70 32.1 New York 11.1 254 86 26.1 North Carolina 13.0 337 45 16.1 North Dakota 0.8 45 44 7.3 Ohio 7.3 120 75 21.4 Oklahoma 6.6 151 68 20.0 Oregon 4.9 159 67 29.3 Pennsylvania 6.3 106 72 14.9 Rhode Island 3.4 174 87 8.3 South Carolina 14.4 279 48 22.5 South Dakota 3.8 86 45 12.8 Tennessee 13.2 188 59 26.9 Texas 12.7 201 80 25.5 Utah 3.2 120 80 22.9 Vermont 2.2 48 32 11.2 Virginia 8.5 156 63 20.7 Washington 4.0 145 73 26.2 West Virginia 5.7 81 39 9.3 Wisconsin 2.6 53 66 10.8 Wyoming 6.8 161 60 15.6 # sometimes (and by sometimes i mean almost always) # there is too much data to display, # in those cases, displaying few top or bottom rows # is a good idea to see what kind of data we have. head(USArrests) # if you want to see more rows from top, you can specify that. head(USArrests, 14) # similarly, you can display few bottom rows, as well, # using the `tail()` function. tail(USArrests) A data.frame: 6 \u00d7 4 Murder Assault UrbanPop Rape <dbl> <int> <int> <dbl> Alabama 13.2 236 58 21.2 Alaska 10.0 263 48 44.5 Arizona 8.1 294 80 31.0 Arkansas 8.8 190 50 19.5 California 9.0 276 91 40.6 Colorado 7.9 204 78 38.7 A data.frame: 14 \u00d7 4 Murder Assault UrbanPop Rape <dbl> <int> <int> <dbl> Alabama 13.2 236 58 21.2 Alaska 10.0 263 48 44.5 Arizona 8.1 294 80 31.0 Arkansas 8.8 190 50 19.5 California 9.0 276 91 40.6 Colorado 7.9 204 78 38.7 Connecticut 3.3 110 77 11.1 Delaware 5.9 238 72 15.8 Florida 15.4 335 80 31.9 Georgia 17.4 211 60 25.8 Hawaii 5.3 46 83 20.2 Idaho 2.6 120 54 14.2 Illinois 10.4 249 83 24.0 Indiana 7.2 113 65 21.0 A data.frame: 6 \u00d7 4 Murder Assault UrbanPop Rape <dbl> <int> <int> <dbl> Vermont 2.2 48 32 11.2 Virginia 8.5 156 63 20.7 Washington 4.0 145 73 26.2 West Virginia 5.7 81 39 9.3 Wisconsin 2.6 53 66 10.8 Wyoming 6.8 161 60 15.6 Since we're using RStudio , we can see the information below easily. In case, we want to display those information in the console or compute with them, this is how it's done: ## size/summary of data # str short for structure # if you prefer to work with matrices: arrests = as.matrix(USArrests) str(arrests) num [1:50, 1:4] 13.2 10 8.1 8.8 9 7.9 3.3 5.9 15.4 17.4 ... - attr(*, \"dimnames\")=List of 2 ..$ : chr [1:50] \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ... ..$ : chr [1:4] \"Murder\" \"Assault\" \"UrbanPop\" \"Rape\"","title":"Example 9"},{"location":"chapter_1_code_snippets/#example-10","text":"Summary statistics and basic visualization. summary(USArrests) Murder Assault UrbanPop Rape Min. : 0.800 Min. : 45.0 Min. :32.00 Min. : 7.30 1st Qu.: 4.075 1st Qu.:109.0 1st Qu.:54.50 1st Qu.:15.07 Median : 7.250 Median :159.0 Median :66.00 Median :20.10 Mean : 7.788 Mean :170.8 Mean :65.54 Mean :21.23 3rd Qu.:11.250 3rd Qu.:249.0 3rd Qu.:77.75 3rd Qu.:26.18 Max. :17.400 Max. :337.0 Max. :91.00 Max. :46.00 # extracting data USArrests[\"California\", \"Murder\"] USArrests[\"California\", ] USArrests[, \"Murder\"] USArrests$Murder 9 A data.frame: 1 \u00d7 4 Murder Assault UrbanPop Rape <dbl> <int> <int> <dbl> California 9 276 91 40.6 .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 13.2 10 8.1 8.8 9 7.9 3.3 5.9 15.4 17.4 5.3 2.6 10.4 7.2 2.2 6 9.7 15.4 2.1 11.3 4.4 12.1 2.7 16.1 9 6 4.3 12.2 2.1 7.4 11.4 11.1 13 0.8 7.3 6.6 4.9 6.3 3.4 14.4 3.8 13.2 12.7 3.2 2.2 8.5 4 5.7 2.6 6.8 .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 13.2 10 8.1 8.8 9 7.9 3.3 5.9 15.4 17.4 5.3 2.6 10.4 7.2 2.2 6 9.7 15.4 2.1 11.3 4.4 12.1 2.7 16.1 9 6 4.3 12.2 2.1 7.4 11.4 11.1 13 0.8 7.3 6.6 4.9 6.3 3.4 14.4 3.8 13.2 12.7 3.2 2.2 8.5 4 5.7 2.6 6.8 # to create a frequency histogram hist(USArrests[, \"Murder\"]) hist(USArrests$Murder) # to create a probability (relative frequency) histogram library(MASS) truehist(USArrests$Murder) hist(USArrests$Murder, prob=TRUE, breaks=\"scott\") USArrests$Murder .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 13.2 10 8.1 8.8 9 7.9 3.3 5.9 15.4 17.4 5.3 2.6 10.4 7.2 2.2 6 9.7 15.4 2.1 11.3 4.4 12.1 2.7 16.1 9 6 4.3 12.2 2.1 7.4 11.4 11.1 13 0.8 7.3 6.6 4.9 6.3 3.4 14.4 3.8 13.2 12.7 3.2 2.2 8.5 4 5.7 2.6 6.8","title":"Example 10"},{"location":"chapter_1_code_snippets/#attaching-a-data-frame","text":"# to save time, sometimes we attach a data frame so that we can use the # row/column names directly without the need of slicing or $ sign. attach(USArrests) # then we can call Murder instead of USArrests$Murder murder.pct = 100 * Murder / (Murder + Assault + Rape) # to detach detach(USArrests)","title":"Attaching a data frame"},{"location":"chapter_1_code_snippets/#scatterplots-and-correlations","text":"attach(USArrests) plot(UrbanPop, Murder) pairs(USArrests) cor(UrbanPop, Murder) 0.0695726217359934 cor(USArrests) A matrix: 4 \u00d7 4 of type dbl Murder Assault UrbanPop Rape Murder 1.00000000 0.8018733 0.06957262 0.5635788 Assault 0.80187331 1.0000000 0.25887170 0.6652412 UrbanPop 0.06957262 0.2588717 1.00000000 0.4113412 Rape 0.56357883 0.6652412 0.41134124 1.0000000","title":"Scatterplots and correlations"},{"location":"chapter_1_code_snippets/#importing-data","text":"You can following tutorial for more detail: http://www.r-tutor.com/r-introduction/data-frame/data-import However, a quick way to import a csv file as a data frame in RStudio is as follows: 1. Download and know where the csv file is. 2. Use the read.csv() function, and put file.choose() as an argument. RStudio will open a window that will let you find the file you want to import. If your csv file has headers, then please also put the argument header=T into the read.csv() function. data <- read.csv(file.choose(), header=T) The most common error is you obtain only one column. The reason is usually that you didn't choose the correct delimiter (what character seperates the values in a row). By looking at the data frame, you can find out what delimiter was to be used. The common choices are: - ',' (comma) - ';' (semicolon) - '\\t' (tab) # to import a csv file with header, where the values in a row are seperated by tab: data = read.csv(file.choose(), sep='\\t', header = T)","title":"Importing Data"},{"location":"homework/","text":"Homework Assignments Final Part II of II Here are some interesting questions summarizing what we did in this class so far. Homework 2 (Final Part I of II) Instructions You can work in pairs as listed below. One member of each team is to send their solutions (the R code, explanation and interpretation of the results) to me. Please send your solutions in a private chat and don't use the main chat. Due time: Before 5pm on July 13. During the first half of our lecture, we will compare and discuss your solutions in class. Teams Team number Members of the team 1 Tatiana & Daniela 2 Emma & Kate 3 Dekeiya & Ritchele 4 Kristen & Mallory 5 Venus & Deirdre 6 Hannah & Jasmine 7 Fariha & Sema Problems Problems 1, 2, 3 and 4 in Chapter 6 of our textbook, R by Example . You can find the data in the list: https://github.com/mariarizzo/RbyExample/tree/master/Rx-data If you have difficulties importing the data, feel free to ask questions in the main chat. Homework 1 The purpose of this week's hw is to make sure the following arrangement works and you find ways to communicate (e.g. private Telegram chats). Teams and their tasks Task one is doing the problems and writing R code and saving the results on a file. The file can be a MS Word file, it can be an R script created in RStudio that contains comment lines explaining the work. Or some other reasonable way where the supervisor team can read, replicate (if necessary) and understand the work. Then the supervisor team looks at the work (put yourselves into my shoes, you're active teachers, hence you know how to evaluate work). If your supervising the work of another team, you're expected to be as critical as possible. Ask questions like \"Is it clarified why this step is taken?\" or \"Why is this variable created? It's not used later.\" or more serious issues like, \"The data frame is wrong, the second columns should've been inverted.\" Let me make something clear, don't worry about the grades, it's all about producing good solutions. Try to produce your best work (with nice explanations to make the supervising team's life easy and to impress them) and as a supervisor team, try to give as much feedback as possible and improve the quality of the solution (which includes clarifying claims and correcting mistakes). Team number Members of the team Gives feedback to the team Which HW problems to solve 1 Tatiana & Emma 7 1, 5 2 Dekeiya & Kristen 1 7, 6 3 Venus & Hannah 2 2, 3 4 Fariha & Mallory 3 4, 1 5 Deirdre & Jasmine 4 5, 7 6 Sema & Daniela 5 6, 2 7 Kate & Ritchele 6 3, 4 # for those who are interested, this is how i randomized the hw assignments for each team set.seed(42) # setting a seed makes a randomization process reproducible problems <- 1:7 # creates a vector (1,2,3,4,5,6,7) problems_shuffled <- sample(problems) # samples from problems vector, i.e. picks 7 numbers from problems randomly rep(problems_shuffled,2) # since there are 14 students and 7 problems, i duplicated the shuffled problem numbers .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 1 5 7 6 2 3 4 1 5 7 6 2 3 4 Due dates Tuesday, June 14, 11:59pm: Finish your problems and send your solutions both to me and to a member of the supervisor team. Thursday, June 16, 11:59pm: Finish evaluating the other pair's solutions and send you feedback to me and back to a member of the other team. Let me give a few hypothetical examples (The teams and the problem assignments are accurate except the last example): - Team 3 does problems 2 and 3. Team 4 supervises team 3. Venus does problem 2, Hannah does problem 3. They take a look at each other's solutions and make sure everything is right before sending them to the supervisor pair (which is team 4). Team 4 looks at the solutions and writes as feedback that one of the graphs doesn't look nice, because the horizontal values are not clear (maybe no labeling), questions the fact that it's not clear where the data comes from, comments that the results are not clearly written. Team 7 does problems 3 and 4. Kate missed the relevant lectures, because she had to save someone in the school library who was having a heart attack. Ritchele being a great person does both of those problems and even explains her solutions to Kate. Kate brings the super expensive chocolate box she received from the person who she saved to Ritchele and lets her to take a few chocolate pieces. Team 1 (being the supervising team) writes the mean is used instead of the median even when the data is quite skew-symmetric and has outliers. Jack in team 12 hates this course. Mike in team 12 went for a vacation. They didn't do the homework. Beyonce and Sting in team 13 do the problems of team 12 because they are perfect. Team 13 gets highly deserved appreciation for that by me and the classmates, we applaud their diligent work in class. Problems Pick any data set that comes with R , describe what the data set contains shortly. Pick a numerical column. Compute the mean and standard deviation of that column. Draw a bar graph/histogram for that column. Draw a bar graph that has only one bar located at the mean (of the chose column). Draw a bar chart that has two bars of height 0.5 located at \"mean - standard deviation\" and \"mean + standard deviation\". Either finish reading chapter 1 and pick an example we didn't discuss in class yet. Explain what makes that example interesting for you. And summarize/comment on the takeaways from that example. Or pick any exercise at the end of chapter 1. Explain the question and your answer to someone who missed the first few lectures of our course. Create a data frame that contains some fake data: A question is asked twice to 20 people and the possible answers are 0, 1, 2, 3. Create two vectors that contain the answers of when the question asked the first time and when the question is asked the second time. Draw the results on two different bar charts. Then find out how to draw the bars together in one bar chart (google the phrase \"stacked bar graph\"). Describe what the difference between a matrix and a data frame. Give an instructive example using numbers and text you pick. Do a few things with that matrix and the data frame (e.g. compute the mean of a column, display the first two rows etc.). Learn and explain how to import a (comma-seperated-values) csv file into a data frame in R . Find a csv file online (just google for csv example or look at this website: https://wsform.com/knowledgebase/sample-csv-files/ ). Download the file to your computer. Then use RStudio to import the file. Explain shortly what Euclidean norm is and write a function that computes the Euclidean length (norm) of a vector (exc 1.10 in the textbook). ~Explain the difference between bar plots and histograms. Give examples for each that illustrates the difference. You can pick columns from data sets that come with R , or go with numbers you arbitrarily pick.~ Nah, do exercise 1.14 in chapter 1 instead.","title":"Homework"},{"location":"homework/#homework-assignments","text":"","title":"Homework Assignments"},{"location":"homework/#final-part-ii-of-ii","text":"Here are some interesting questions summarizing what we did in this class so far.","title":"Final Part II of II"},{"location":"homework/#homework-2-final-part-i-of-ii","text":"","title":"Homework 2 (Final Part I of II)"},{"location":"homework/#instructions","text":"You can work in pairs as listed below. One member of each team is to send their solutions (the R code, explanation and interpretation of the results) to me. Please send your solutions in a private chat and don't use the main chat. Due time: Before 5pm on July 13. During the first half of our lecture, we will compare and discuss your solutions in class.","title":"Instructions"},{"location":"homework/#teams","text":"Team number Members of the team 1 Tatiana & Daniela 2 Emma & Kate 3 Dekeiya & Ritchele 4 Kristen & Mallory 5 Venus & Deirdre 6 Hannah & Jasmine 7 Fariha & Sema","title":"Teams"},{"location":"homework/#problems","text":"Problems 1, 2, 3 and 4 in Chapter 6 of our textbook, R by Example . You can find the data in the list: https://github.com/mariarizzo/RbyExample/tree/master/Rx-data If you have difficulties importing the data, feel free to ask questions in the main chat.","title":"Problems"},{"location":"homework/#homework-1","text":"The purpose of this week's hw is to make sure the following arrangement works and you find ways to communicate (e.g. private Telegram chats).","title":"Homework 1"},{"location":"homework/#teams-and-their-tasks","text":"Task one is doing the problems and writing R code and saving the results on a file. The file can be a MS Word file, it can be an R script created in RStudio that contains comment lines explaining the work. Or some other reasonable way where the supervisor team can read, replicate (if necessary) and understand the work. Then the supervisor team looks at the work (put yourselves into my shoes, you're active teachers, hence you know how to evaluate work). If your supervising the work of another team, you're expected to be as critical as possible. Ask questions like \"Is it clarified why this step is taken?\" or \"Why is this variable created? It's not used later.\" or more serious issues like, \"The data frame is wrong, the second columns should've been inverted.\" Let me make something clear, don't worry about the grades, it's all about producing good solutions. Try to produce your best work (with nice explanations to make the supervising team's life easy and to impress them) and as a supervisor team, try to give as much feedback as possible and improve the quality of the solution (which includes clarifying claims and correcting mistakes). Team number Members of the team Gives feedback to the team Which HW problems to solve 1 Tatiana & Emma 7 1, 5 2 Dekeiya & Kristen 1 7, 6 3 Venus & Hannah 2 2, 3 4 Fariha & Mallory 3 4, 1 5 Deirdre & Jasmine 4 5, 7 6 Sema & Daniela 5 6, 2 7 Kate & Ritchele 6 3, 4 # for those who are interested, this is how i randomized the hw assignments for each team set.seed(42) # setting a seed makes a randomization process reproducible problems <- 1:7 # creates a vector (1,2,3,4,5,6,7) problems_shuffled <- sample(problems) # samples from problems vector, i.e. picks 7 numbers from problems randomly rep(problems_shuffled,2) # since there are 14 students and 7 problems, i duplicated the shuffled problem numbers .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 1 5 7 6 2 3 4 1 5 7 6 2 3 4","title":"Teams and their tasks"},{"location":"homework/#due-dates","text":"Tuesday, June 14, 11:59pm: Finish your problems and send your solutions both to me and to a member of the supervisor team. Thursday, June 16, 11:59pm: Finish evaluating the other pair's solutions and send you feedback to me and back to a member of the other team. Let me give a few hypothetical examples (The teams and the problem assignments are accurate except the last example): - Team 3 does problems 2 and 3. Team 4 supervises team 3. Venus does problem 2, Hannah does problem 3. They take a look at each other's solutions and make sure everything is right before sending them to the supervisor pair (which is team 4). Team 4 looks at the solutions and writes as feedback that one of the graphs doesn't look nice, because the horizontal values are not clear (maybe no labeling), questions the fact that it's not clear where the data comes from, comments that the results are not clearly written. Team 7 does problems 3 and 4. Kate missed the relevant lectures, because she had to save someone in the school library who was having a heart attack. Ritchele being a great person does both of those problems and even explains her solutions to Kate. Kate brings the super expensive chocolate box she received from the person who she saved to Ritchele and lets her to take a few chocolate pieces. Team 1 (being the supervising team) writes the mean is used instead of the median even when the data is quite skew-symmetric and has outliers. Jack in team 12 hates this course. Mike in team 12 went for a vacation. They didn't do the homework. Beyonce and Sting in team 13 do the problems of team 12 because they are perfect. Team 13 gets highly deserved appreciation for that by me and the classmates, we applaud their diligent work in class.","title":"Due dates"},{"location":"homework/#problems_1","text":"Pick any data set that comes with R , describe what the data set contains shortly. Pick a numerical column. Compute the mean and standard deviation of that column. Draw a bar graph/histogram for that column. Draw a bar graph that has only one bar located at the mean (of the chose column). Draw a bar chart that has two bars of height 0.5 located at \"mean - standard deviation\" and \"mean + standard deviation\". Either finish reading chapter 1 and pick an example we didn't discuss in class yet. Explain what makes that example interesting for you. And summarize/comment on the takeaways from that example. Or pick any exercise at the end of chapter 1. Explain the question and your answer to someone who missed the first few lectures of our course. Create a data frame that contains some fake data: A question is asked twice to 20 people and the possible answers are 0, 1, 2, 3. Create two vectors that contain the answers of when the question asked the first time and when the question is asked the second time. Draw the results on two different bar charts. Then find out how to draw the bars together in one bar chart (google the phrase \"stacked bar graph\"). Describe what the difference between a matrix and a data frame. Give an instructive example using numbers and text you pick. Do a few things with that matrix and the data frame (e.g. compute the mean of a column, display the first two rows etc.). Learn and explain how to import a (comma-seperated-values) csv file into a data frame in R . Find a csv file online (just google for csv example or look at this website: https://wsform.com/knowledgebase/sample-csv-files/ ). Download the file to your computer. Then use RStudio to import the file. Explain shortly what Euclidean norm is and write a function that computes the Euclidean length (norm) of a vector (exc 1.10 in the textbook). ~Explain the difference between bar plots and histograms. Give examples for each that illustrates the difference. You can pick columns from data sets that come with R , or go with numbers you arbitrarily pick.~ Nah, do exercise 1.14 in chapter 1 instead.","title":"Problems"},{"location":"intro_to_r/","text":"Lecture 1: Preliminaries R and RStudio R is a statistical computing environment: open source and free software for statistical computation and graphics and a computer language designed for typical statistical and graphical applications R is an interpreted language: You type some commands (via at terminal/command prompt or from a file called a script), and R processes those commands. In this course, we will use RStudio to interact with R . RStudio is an integrated development environment, in other words user friendly software to interact with a language. How to install R and RStudio You can install R from: The Comprehensive R Archive Network You can install RStudio from: Download the RStudio IDE - RStudio Here's a webpage that explains the installation process in steps for different operating systems: Install R and RStudio - A Step-by-Step Guide for Beginners - TechVidvan Here's a Youtube tutorial for MacOS: How to install R and RStudio on Mac - YouTube Here's a Youtube tutorial for Windows: How to download and install R and RStudio - YouTube Basic calculations in R You can use R as a calculator and let it process a one line operation: 1+2 3 Here's a tidy website that contains a list of useful operators: R Operators - Learn By Example In this course, we'll be doing more complex computations and we'll need to save the result of a computation to be used later in some other computation. For that reason, we have variables . You can think of a variable as a keyword you chose to store a particular piece of information (numerical, or text or even logical). In R , there are two ways to assign a value to a variable: using = and using <- First you type the name of the variable you chose, then you type = or <- and then you put what value you want to assign to that variable. That value can be the result of some calculation, can be the output of some function, it can be a number or text or logical value you want to store. # note that, when you make an assignment, you don't get an output. # if you're using RStudio, the variable and its value can be seen in the Environment pane a = 3 ^ 2 #(example: exponentiation) # if you want R to return the value of a variable to the console, you'll need to type the name a 9 b <- 8 %/% 3 #(example: integer division) b 2 # you can see the variables in RStudio in a nice, organized way in the Environment pane # but if you want to see a list of variables in the Console: ls() 'a' 'b' Note that R is case sensitive, for example a and A are treated as different variable names. A = 7 a 9 A 7 If you want to update the value of a variable, you can do that of course. A = 10 # now the value of A is not 7 anymore, but 10. A 10 Basic Data Types in R Here's a nice reference for more on basic data types and data structures in R : https://swcarpentry.github.io/r-novice-inflammation/13-supp-data-structures/ There are 6 data types in R : character numeric (real or decimal) integer logical complex You can find out the type of an object using the typeof() function. We'll discuss functions below, but basically they are self-contained algorithms built to perform a specific task. A function expects a user to give arguments as input and based on the values of the arguments, a function produces an output. # character values are anything that is to be treated as text typeof(\"ab\") 'character' In the example above, we gave \"ab\" as the input to the function typeof() by typing \"ab\" between the parentheses, and the typeof() function returned us the type of \"ab\" as the output. # R will decide how to store the value unless you specify the data type manually typeof(2) 'double' (If you wonder why the word 'double', double just means a decimal number in double precision, i.e. two decimals after the decimal point.) # by using '' or \"\", we can tell R that we're using a characters typeof('2') 'character' You can store text in a variable as well. To store a text value, you'll need to use \" or ' . Otherwise, R thinks you're typing the name of a variable. # note the the ' signs between the \" signs are treated as apostrophes: mytext <- \"I'm having a great 'day' today.\" mytext 'I\\'m having a great \\'day\\' today.' The backslash before the apostrophe \\' is R 's way of remembering that ' sign is to be treated as an apostrophe, but not as a delimiter of some text data. # the backslashes above are to denote escape sequences # \\' tells R to treat the apostrophe as a character # to print nicely, you can use the cat() function cat(mytext) I'm having a great 'day' today. typeof(2L) # the L on the right tells R to store 2 as an integer 'integer' typeof(-12.4) 'double' typeof(T) 'logical' typeof(FALSE) 'logical' typeof(1+4i) 'complex' Basic Data Structures in R Data structures are ways to store (usually multiple) values together in a specific way. R has multiple data structures. The most important ones for us include: atomic vector list matrix data frame factors # you can think of vectors as lists/arrays # that contain items of the same type # the function c() below combines items to create vectors vec1 <- c(1,2,3,20,3) vec2 <- c(T,T,F,F,FALSE, TRUE) vec3 <- c('a','stats', '614', \"\\'\") vec4 <- c(2L, -5L) vec5 <- 1:10 length(vec1) 5 typeof(vec2) 'logical' typeof(vec3) 'character' cat(vec4) 2 -5 2*vec4 4 -10 vec5 1 2 3 4 5 6 7 8 9 10 vec6 <- vec5 - vec5 vec6 0 0 0 0 0 0 0 0 0 0 vec4 + vec6 2 -5 2 -5 2 -5 2 -5 2 -5 vec7 <- c(1,2,3) typeof(vec7) 'double' vec8 <- c(1L,2L,3L) typeof(vec8) 'integer' # in R, == means comparing and asking if the items are equal vec7 == vec8 TRUE TRUE TRUE # the str() function compactly displayes the internal STRucture of an R object # rdocumentation.org/packages/utils/versions/3.6.2/topics/str str(vec8) int [1:3] 1 2 3 How to add a new element # below, i'm combining vec8 and the number 4 as an integer, # the number 4 is appended on the right, # hence the last item of vec9 is 4 vec9 <- c(vec8,4L) vec9 1 2 3 4 # below, the number zero is appended to vec9 on the left, # i.e. the first item in vec10 is 0 vec10 <- c(0L,vec9) vec10 0 1 2 3 4 How to create vectors from a sequence of numbers # the 1:10 command creates a sequence of integers that start at 1 and end at 10 vec11 <- 1:10 vec11 1 2 3 4 5 6 7 8 9 10 # you can create sequences more directly using the seq() function # you specify where the sequence of numbers start, end and the stepsize. vec12 <- seq(from = 1, to = 2, by = 0.1) vec12 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 # if the stepsize is not given, the default stepsize is used, which is 1. vec12 <- seq(from = 1, to = 20) vec12 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Factors Factors are categorical variables in R . We'll talk about categorical variables and factors later, but as foreshadowing, we can use categorical variables to create hierarchies in the sense that we can compute the mean age of everyone, and at a detailed level, we can compute the mean age of female and male people using \"Female\" and \"Male\" categories. vec13 = as.factor(c(\"Male\", \"Female\", \"Male\", \"Female\")) vec13 Male Female Male Female Levels : 'Female' 'Male' # note that R automatically decides what the underlying data type is when creating a vector # the items in a vector need to be of the same data type. vec14 = c(1,TRUE,'A') class(vec14) 'character' Lists # what if we want to store items of different data types and keep the data types as they are? # we use lists mylist <- list(1,TRUE,'A') mylist 1 TRUE 'A' typeof(mylist) 'list' mylist2 <- list(mylist, c(1,2,3)) mylist2 1 TRUE 'A' 1 2 3 mylist3 <- c(mylist, c(1,2,3)) mylist3 1 TRUE 'A' 1 2 3 Data Frames You can think of data frames as lists of vectors of equal length forming columns. Or even better, like a spreadsheet. # to create a data frame from vectors, you can use the data.frame() function data.frame(vec1,vec2,vec3) Error in data.frame(vec1, vec2, vec3): arguments imply differing number of rows: 5, 6, 4 Traceback: 1. data.frame(vec1, vec2, vec3) 2. stop(gettextf(\"arguments imply differing number of rows: %s\", . paste(unique(nrows), collapse = \", \")), domain = NA) # since those vectors were of different lengths, R couldn't create a data frame. # below, i use length() and cat() functions to return the lengths of the vectors we tried to combine to a data frame above. cat(length(vec1),length(vec2),length(vec3)) 5 6 4 vec15 <- c(vec1,-1) vec16 <- c('one', \"two\", vec3) # note that vec15, vec2 and vec16 have the same length after modifying them for the sake of this example. cat(length(vec15),length(vec2),length(vec16)) 6 6 6 # we can create the data frame. data.frame(vec15,vec2,vec16) A data.frame: 6 \u00d7 3 vec15 vec2 vec16 1 TRUE one 2 TRUE two 3 FALSE a 20 FALSE stats 3 FALSE 614 -1 TRUE ' # it's customary to denote the variable that represents a data frame # by the variable name df df <- data.frame(a = vec15,b = vec2,c = vec16, d = vec2 + vec15) df A data.frame: 6 \u00d7 4 a b c d 1 TRUE one 2 2 TRUE two 3 3 FALSE a 3 20 FALSE stats 20 3 FALSE 614 3 -1 TRUE ' 0 Matrices # the matrix function creates a matrix below that contains 3 rows and 2 columns # the difference of a matrix is that the data types of the items in a matrix need to be equal # you can think of matrices as a 2D generalization of vectors. mymatrix = matrix(vec16, 3, 2) mymatrix A matrix: 3 \u00d7 2 of type chr one stats two 614 a ' # finding the transpose matrix: columns are converted to rows and rows to columns. # for example, the first column of the matrix above is the first row of the transpose below. t(mymatrix) A matrix: 2 \u00d7 3 of type chr one two a stats 614 ' Indexing # how to grab the second value of a vector vec3[2] 'stats' vec3 'a' 'stats' '614' '\\'' 1:3 # means all the numbers between 1 and 3 (1 and 3 included) 1 2 3 # what if we want more values vec3[1:3] 'a' 'stats' '614' vec3[-1] 'stats' '614' '\\'' Filtering using Boolean values length(vec3) 4 vec3[c(T,T,F,T)] 'a' 'stats' '\\'' vec3[c(1,2,4)] 'a' 'stats' '\\'' vec3[5] NA # similar with lists, but remember: you get sublists mylist 1 TRUE 'A' mylist[1:2] 1 TRUE mylist[1] 1 typeof(mylist[1]) 'list' # to get the actual items mylist[[1]] 1 # giving names to items mylist4 <- list(\"a\" = vec1, \"vec2\" = vec2, namewithouthquotes = vec3, mynumber = 20, list(vec1, vec2, vec3), mean) mylist4 $a 1 2 3 20 3 $vec2 TRUE TRUE FALSE FALSE FALSE TRUE $namewithouthquotes 'a' 'stats' '614' '\\'' $mynumber 20 [[5]] 1 2 3 20 3 TRUE TRUE FALSE FALSE FALSE TRUE 'a' 'stats' '614' '\\'' [[6]] function (x, ...) UseMethod(\"mean\") # we gave names to items in the list names(mylist4) 'a' 'vec2' 'namewithouthquotes' 'mynumber' '' '' mylist4[1] 1 2 3 20 3 mylist4[\"a\"] 1 2 3 20 3 Slicing/indexing data frames df A data.frame: 6 \u00d7 4 a b c d 1 TRUE one 2 2 TRUE two 3 3 FALSE a 3 20 FALSE stats 20 3 FALSE 614 3 -1 TRUE ' 0 df$a 1 2 3 20 3 -1 df[\"a\"] A data.frame: 6 \u00d7 1 a 1 2 3 20 3 -1 df[[\"a\"]] 1 2 3 20 3 -1 df[[1]] 1 2 3 20 3 -1 df[1][4] Error in `[.data.frame`(df[1], 4): undefined columns selected Traceback: 1. df[1][4] 2. `[.data.frame`(df[1], 4) 3. stop(\"undefined columns selected\") # 1st column 4th row df[[1]][4] 20 df[[1]][4:6] 20 3 -1 # first row 4th column df[1,4] 2 df[4,1] 20 df[1,] A data.frame: 1 \u00d7 4 a b c d 1 1 TRUE one 2 df[c(1,2,4),] A data.frame: 3 \u00d7 4 a b c d 1 1 TRUE one 2 2 2 TRUE two 3 4 20 FALSE stats 20 mymatrix A matrix: 3 \u00d7 2 of type chr one stats two 614 a ' mymatrix[1,2] 'stats' Functions new_sum <- function(value1, value2){ results <- value1 + value2 return(results) } new_sum(2,4) 6 new_division <- function(x,y){ return(y/x) } new_division(5,4) 0.8 new_division(y=4,x=5) 0.8 new_division function (x, y) { return(y/x) } ?mean Packages # installing a library install.packages(\"ggplot2\") Updating HTML index of packages in '.Library' Making 'packages.html' ... done # importing a library library(ggplot2) # once you import a library, you can call any function from that library ggplot(df, aes(a,b)) + geom_point()","title":"00 - Intro to R"},{"location":"intro_to_r/#lecture-1-preliminaries","text":"","title":"Lecture 1: Preliminaries"},{"location":"intro_to_r/#r-and-rstudio","text":"R is a statistical computing environment: open source and free software for statistical computation and graphics and a computer language designed for typical statistical and graphical applications R is an interpreted language: You type some commands (via at terminal/command prompt or from a file called a script), and R processes those commands. In this course, we will use RStudio to interact with R . RStudio is an integrated development environment, in other words user friendly software to interact with a language.","title":"R and RStudio"},{"location":"intro_to_r/#how-to-install-r-and-rstudio","text":"You can install R from: The Comprehensive R Archive Network You can install RStudio from: Download the RStudio IDE - RStudio Here's a webpage that explains the installation process in steps for different operating systems: Install R and RStudio - A Step-by-Step Guide for Beginners - TechVidvan Here's a Youtube tutorial for MacOS: How to install R and RStudio on Mac - YouTube Here's a Youtube tutorial for Windows: How to download and install R and RStudio - YouTube","title":"How to install R and RStudio"},{"location":"intro_to_r/#basic-calculations-in-r","text":"You can use R as a calculator and let it process a one line operation: 1+2 3 Here's a tidy website that contains a list of useful operators: R Operators - Learn By Example In this course, we'll be doing more complex computations and we'll need to save the result of a computation to be used later in some other computation. For that reason, we have variables . You can think of a variable as a keyword you chose to store a particular piece of information (numerical, or text or even logical). In R , there are two ways to assign a value to a variable: using = and using <- First you type the name of the variable you chose, then you type = or <- and then you put what value you want to assign to that variable. That value can be the result of some calculation, can be the output of some function, it can be a number or text or logical value you want to store. # note that, when you make an assignment, you don't get an output. # if you're using RStudio, the variable and its value can be seen in the Environment pane a = 3 ^ 2 #(example: exponentiation) # if you want R to return the value of a variable to the console, you'll need to type the name a 9 b <- 8 %/% 3 #(example: integer division) b 2 # you can see the variables in RStudio in a nice, organized way in the Environment pane # but if you want to see a list of variables in the Console: ls() 'a' 'b' Note that R is case sensitive, for example a and A are treated as different variable names. A = 7 a 9 A 7 If you want to update the value of a variable, you can do that of course. A = 10 # now the value of A is not 7 anymore, but 10. A 10","title":"Basic calculations in R"},{"location":"intro_to_r/#basic-data-types-in-r","text":"Here's a nice reference for more on basic data types and data structures in R : https://swcarpentry.github.io/r-novice-inflammation/13-supp-data-structures/ There are 6 data types in R : character numeric (real or decimal) integer logical complex You can find out the type of an object using the typeof() function. We'll discuss functions below, but basically they are self-contained algorithms built to perform a specific task. A function expects a user to give arguments as input and based on the values of the arguments, a function produces an output. # character values are anything that is to be treated as text typeof(\"ab\") 'character' In the example above, we gave \"ab\" as the input to the function typeof() by typing \"ab\" between the parentheses, and the typeof() function returned us the type of \"ab\" as the output. # R will decide how to store the value unless you specify the data type manually typeof(2) 'double' (If you wonder why the word 'double', double just means a decimal number in double precision, i.e. two decimals after the decimal point.) # by using '' or \"\", we can tell R that we're using a characters typeof('2') 'character' You can store text in a variable as well. To store a text value, you'll need to use \" or ' . Otherwise, R thinks you're typing the name of a variable. # note the the ' signs between the \" signs are treated as apostrophes: mytext <- \"I'm having a great 'day' today.\" mytext 'I\\'m having a great \\'day\\' today.' The backslash before the apostrophe \\' is R 's way of remembering that ' sign is to be treated as an apostrophe, but not as a delimiter of some text data. # the backslashes above are to denote escape sequences # \\' tells R to treat the apostrophe as a character # to print nicely, you can use the cat() function cat(mytext) I'm having a great 'day' today. typeof(2L) # the L on the right tells R to store 2 as an integer 'integer' typeof(-12.4) 'double' typeof(T) 'logical' typeof(FALSE) 'logical' typeof(1+4i) 'complex'","title":"Basic Data Types in R"},{"location":"intro_to_r/#basic-data-structures-in-r","text":"Data structures are ways to store (usually multiple) values together in a specific way. R has multiple data structures. The most important ones for us include: atomic vector list matrix data frame factors # you can think of vectors as lists/arrays # that contain items of the same type # the function c() below combines items to create vectors vec1 <- c(1,2,3,20,3) vec2 <- c(T,T,F,F,FALSE, TRUE) vec3 <- c('a','stats', '614', \"\\'\") vec4 <- c(2L, -5L) vec5 <- 1:10 length(vec1) 5 typeof(vec2) 'logical' typeof(vec3) 'character' cat(vec4) 2 -5 2*vec4 4 -10 vec5 1 2 3 4 5 6 7 8 9 10 vec6 <- vec5 - vec5 vec6 0 0 0 0 0 0 0 0 0 0 vec4 + vec6 2 -5 2 -5 2 -5 2 -5 2 -5 vec7 <- c(1,2,3) typeof(vec7) 'double' vec8 <- c(1L,2L,3L) typeof(vec8) 'integer' # in R, == means comparing and asking if the items are equal vec7 == vec8 TRUE TRUE TRUE # the str() function compactly displayes the internal STRucture of an R object # rdocumentation.org/packages/utils/versions/3.6.2/topics/str str(vec8) int [1:3] 1 2 3","title":"Basic Data Structures in R"},{"location":"intro_to_r/#how-to-add-a-new-element","text":"# below, i'm combining vec8 and the number 4 as an integer, # the number 4 is appended on the right, # hence the last item of vec9 is 4 vec9 <- c(vec8,4L) vec9 1 2 3 4 # below, the number zero is appended to vec9 on the left, # i.e. the first item in vec10 is 0 vec10 <- c(0L,vec9) vec10 0 1 2 3 4","title":"How to add a new element"},{"location":"intro_to_r/#how-to-create-vectors-from-a-sequence-of-numbers","text":"# the 1:10 command creates a sequence of integers that start at 1 and end at 10 vec11 <- 1:10 vec11 1 2 3 4 5 6 7 8 9 10 # you can create sequences more directly using the seq() function # you specify where the sequence of numbers start, end and the stepsize. vec12 <- seq(from = 1, to = 2, by = 0.1) vec12 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 # if the stepsize is not given, the default stepsize is used, which is 1. vec12 <- seq(from = 1, to = 20) vec12 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20","title":"How to create vectors from a sequence of numbers"},{"location":"intro_to_r/#factors","text":"Factors are categorical variables in R . We'll talk about categorical variables and factors later, but as foreshadowing, we can use categorical variables to create hierarchies in the sense that we can compute the mean age of everyone, and at a detailed level, we can compute the mean age of female and male people using \"Female\" and \"Male\" categories. vec13 = as.factor(c(\"Male\", \"Female\", \"Male\", \"Female\")) vec13 Male Female Male Female Levels : 'Female' 'Male' # note that R automatically decides what the underlying data type is when creating a vector # the items in a vector need to be of the same data type. vec14 = c(1,TRUE,'A') class(vec14) 'character'","title":"Factors"},{"location":"intro_to_r/#lists","text":"# what if we want to store items of different data types and keep the data types as they are? # we use lists mylist <- list(1,TRUE,'A') mylist 1 TRUE 'A' typeof(mylist) 'list' mylist2 <- list(mylist, c(1,2,3)) mylist2 1 TRUE 'A' 1 2 3 mylist3 <- c(mylist, c(1,2,3)) mylist3 1 TRUE 'A' 1 2 3","title":"Lists"},{"location":"intro_to_r/#data-frames","text":"You can think of data frames as lists of vectors of equal length forming columns. Or even better, like a spreadsheet. # to create a data frame from vectors, you can use the data.frame() function data.frame(vec1,vec2,vec3) Error in data.frame(vec1, vec2, vec3): arguments imply differing number of rows: 5, 6, 4 Traceback: 1. data.frame(vec1, vec2, vec3) 2. stop(gettextf(\"arguments imply differing number of rows: %s\", . paste(unique(nrows), collapse = \", \")), domain = NA) # since those vectors were of different lengths, R couldn't create a data frame. # below, i use length() and cat() functions to return the lengths of the vectors we tried to combine to a data frame above. cat(length(vec1),length(vec2),length(vec3)) 5 6 4 vec15 <- c(vec1,-1) vec16 <- c('one', \"two\", vec3) # note that vec15, vec2 and vec16 have the same length after modifying them for the sake of this example. cat(length(vec15),length(vec2),length(vec16)) 6 6 6 # we can create the data frame. data.frame(vec15,vec2,vec16) A data.frame: 6 \u00d7 3 vec15 vec2 vec16 1 TRUE one 2 TRUE two 3 FALSE a 20 FALSE stats 3 FALSE 614 -1 TRUE ' # it's customary to denote the variable that represents a data frame # by the variable name df df <- data.frame(a = vec15,b = vec2,c = vec16, d = vec2 + vec15) df A data.frame: 6 \u00d7 4 a b c d 1 TRUE one 2 2 TRUE two 3 3 FALSE a 3 20 FALSE stats 20 3 FALSE 614 3 -1 TRUE ' 0","title":"Data Frames"},{"location":"intro_to_r/#matrices","text":"# the matrix function creates a matrix below that contains 3 rows and 2 columns # the difference of a matrix is that the data types of the items in a matrix need to be equal # you can think of matrices as a 2D generalization of vectors. mymatrix = matrix(vec16, 3, 2) mymatrix A matrix: 3 \u00d7 2 of type chr one stats two 614 a ' # finding the transpose matrix: columns are converted to rows and rows to columns. # for example, the first column of the matrix above is the first row of the transpose below. t(mymatrix) A matrix: 2 \u00d7 3 of type chr one two a stats 614 '","title":"Matrices"},{"location":"intro_to_r/#indexing","text":"# how to grab the second value of a vector vec3[2] 'stats' vec3 'a' 'stats' '614' '\\'' 1:3 # means all the numbers between 1 and 3 (1 and 3 included) 1 2 3 # what if we want more values vec3[1:3] 'a' 'stats' '614' vec3[-1] 'stats' '614' '\\''","title":"Indexing"},{"location":"intro_to_r/#filtering-using-boolean-values","text":"length(vec3) 4 vec3[c(T,T,F,T)] 'a' 'stats' '\\'' vec3[c(1,2,4)] 'a' 'stats' '\\'' vec3[5] NA # similar with lists, but remember: you get sublists mylist 1 TRUE 'A' mylist[1:2] 1 TRUE mylist[1] 1 typeof(mylist[1]) 'list' # to get the actual items mylist[[1]] 1 # giving names to items mylist4 <- list(\"a\" = vec1, \"vec2\" = vec2, namewithouthquotes = vec3, mynumber = 20, list(vec1, vec2, vec3), mean) mylist4 $a 1 2 3 20 3 $vec2 TRUE TRUE FALSE FALSE FALSE TRUE $namewithouthquotes 'a' 'stats' '614' '\\'' $mynumber 20 [[5]] 1 2 3 20 3 TRUE TRUE FALSE FALSE FALSE TRUE 'a' 'stats' '614' '\\'' [[6]] function (x, ...) UseMethod(\"mean\") # we gave names to items in the list names(mylist4) 'a' 'vec2' 'namewithouthquotes' 'mynumber' '' '' mylist4[1] 1 2 3 20 3 mylist4[\"a\"] 1 2 3 20 3","title":"Filtering using Boolean values"},{"location":"intro_to_r/#slicingindexing-data-frames","text":"df A data.frame: 6 \u00d7 4 a b c d 1 TRUE one 2 2 TRUE two 3 3 FALSE a 3 20 FALSE stats 20 3 FALSE 614 3 -1 TRUE ' 0 df$a 1 2 3 20 3 -1 df[\"a\"] A data.frame: 6 \u00d7 1 a 1 2 3 20 3 -1 df[[\"a\"]] 1 2 3 20 3 -1 df[[1]] 1 2 3 20 3 -1 df[1][4] Error in `[.data.frame`(df[1], 4): undefined columns selected Traceback: 1. df[1][4] 2. `[.data.frame`(df[1], 4) 3. stop(\"undefined columns selected\") # 1st column 4th row df[[1]][4] 20 df[[1]][4:6] 20 3 -1 # first row 4th column df[1,4] 2 df[4,1] 20 df[1,] A data.frame: 1 \u00d7 4 a b c d 1 1 TRUE one 2 df[c(1,2,4),] A data.frame: 3 \u00d7 4 a b c d 1 1 TRUE one 2 2 2 TRUE two 3 4 20 FALSE stats 20 mymatrix A matrix: 3 \u00d7 2 of type chr one stats two 614 a ' mymatrix[1,2] 'stats'","title":"Slicing/indexing data frames"},{"location":"intro_to_r/#functions","text":"new_sum <- function(value1, value2){ results <- value1 + value2 return(results) } new_sum(2,4) 6 new_division <- function(x,y){ return(y/x) } new_division(5,4) 0.8 new_division(y=4,x=5) 0.8 new_division function (x, y) { return(y/x) } ?mean","title":"Functions"},{"location":"intro_to_r/#packages","text":"# installing a library install.packages(\"ggplot2\") Updating HTML index of packages in '.Library' Making 'packages.html' ... done # importing a library library(ggplot2) # once you import a library, you can call any function from that library ggplot(df, aes(a,b)) + geom_point()","title":"Packages"},{"location":"chapter_2/quantitative/","text":"Chapter 2: Quantitive Data Bivariate Data: Two Quantitive (Numerical) Variables We start with importing a data set library called MASS . It contains the example we will be working with. # import the MASS data set library. library(MASS) This import will enlarge the data set examples in R . To see a list of new data sets that come with the MASS package, you can simply the following command. # list the data sets that come with MASS data(package = 'MASS') Or, simply list of all data sets currently accessable in your session. (The data sets will be sorted in blocks according the package they come with.) # list all data sets in `R` (the updated list) data() Example 1 In this example, we look at the mammals data set. # see how the data frame looks like by inspecting the first few rows. head(mammals) A data.frame: 6 \u00d7 2 body brain <dbl> <dbl> Arctic fox 3.385 44.5 Owl monkey 0.480 15.5 Mountain beaver 1.350 8.1 Cow 465.000 423.0 Grey wolf 36.330 119.5 Goat 27.660 115.0 # how many rows and columns the data set contains. dim(mammals) .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 62 2 After looking at the first few rows, and the dimension of the data frame, we see that the data set mammals contains body and brain sizes of 62 different mammal species. If you want to learn more about the data set mammals , you can check the documentation of the data set by simply typing ?mammals or help(mammals) . # if you want to learn more about the data set, you can check the documentation ?mammals To make sure the format of the data set you can type the following commands: # is the data set stored in a matrix? is.matrix(mammals) FALSE # is the data set stored in a data frame? is.data.frame(mammals) TRUE # or just ask for the class: data frame or matrix? class(mammals) 'data.frame' # to see the answers of most of the questions above, you can simply type str(mammals) 'data.frame': 62 obs. of 2 variables: $ body : num 3.38 0.48 1.35 465 36.33 ... $ brain: num 44.5 15.5 8.1 423 119.5 ... The command summary() summarizes each numerical column of the data frame and gives the frequency distributions of the categorical columns. summary(mammals) body brain Min. : 0.005 Min. : 0.14 1st Qu.: 0.600 1st Qu.: 4.25 Median : 3.342 Median : 17.25 Mean : 198.790 Mean : 283.13 3rd Qu.: 48.202 3rd Qu.: 166.00 Max. :6654.000 Max. :5712.00 The problem with this data set is that the difference between the max and min is huge! Digression What are the smallest and largest animals in the mammals dataset? # which.min() gives the row number that contains the minimum of the column you specify # then to display that row, you can use the [] method mammals[which.min(mammals$body), ] A data.frame: 1 \u00d7 2 body brain <dbl> <dbl> Lesser short-tailed shrew 0.005 0.14 mammals[which.max(mammals$body), ] A data.frame: 1 \u00d7 2 body brain <dbl> <dbl> African elephant 6654 5712 Back to the example: To see a visualization of the 5-number-summary (min, Q1, median, Q2, max), we use boxplots. # to see the boxplot of each numerical column boxplot(mammals) The problem with this visualization is that the most of the mammals are relatively small (compared to a few huge ones), and to fit the bigger mammals into the graph, the boxplots are squeezed down all the way down. You can easy create scatter plots for every pair of variables (our dataset contains two variables body and brain hence there will be only one scatter plot). plot(mammals) Again, most of the points are squeezed down to the bottom left corner due to the existence of the very large mammals such as the elephant. What our textbook suggest is to using the logarithmic scale and see the order of magnitudes of the numbers. For those of you who don't remember much about logarithms: Digression: Logarithms The (natural) logarithm of a (positive) number $b$ is the power you need to raise the Euler number $e$ (which is an irrational number like $\\pi$ that happens to have very nice properties in math) to get $b$: $$\\log(a)= b \\leftrightarrow e^b = a$$ The Euler number $e$ has the approximate decimal value $2.7182$. To display $e$ in R : exp(1) What logarithm computes: log(1000) (exp(1))^6.907755279 # the error is due to rounding. $$\\log(1000)\\approx 6.908 \\leftrightarrow e^{6.908} \\approx 1000$$ You can think of $\\log(1000)$ as the number that tells how big 1000 is multiplicatively (when the standard/unit is taken to be the number $e\\approx 2.7182$.) Back to the example summary(log(mammals)) These numbers tell you how big the values are multiplicatively. This is a neat trick: If the numbers deviate a lot additively (i.e. max - min is too big), you may want to compare the numbers multiplicatively. plot(log(mammals), xlab=\"log(body)\", ylab=\"log(brain)\", las=1) boxplot(log(mammals)) Note that when you think of the numeric data in a multiplicative way, the visualizations look much better. One very important number that summarizes the relationship between two numberical variables is the (Pearson) correlation coefficient. It measures how strongly the values depend on each other: The (Pearson) correlation coefficient $r$ tells us two important things: The sign of the correlation coefficient between the variables tells us if the relation is positive or negative, i.e. if the second variable increases or decreases on average, when the first variable increases. The size of the correlation coefficient tells how strongly the variables are related. If the $|r|$ is close to 1, then the variables are strongly (linearly) related, and if $|r|$ is close to 0, then the strength of the relation between the variables is quite weak, if we can say there is a relation at all. cor(log(mammals)) A matrix: 2 \u00d7 2 of type dbl body brain body 1.0000000 0.9595748 brain 0.9595748 1.0000000 Staying in the log-log scale, we see that brain and body variables have a correlation constant of 0.960, which is pretty close to 1. In other words, we can pretty accurately say, the larger the body size, the larger the brain size for mammals on average. (I know this sounds as a trivial observation, but think of the dinosaurs: dinosaur brain .) If there is a strong linear relation between two numeric variables (which we can find out by looking at the correlation coefficient of them), we can model that relation using a line (the simplest non-trivial model). # to simplify the prompts, first give log() values new names x = log(mammals$body) y = log(mammals$brain) # gives the best fitting line #when x is taken as the independent variable and y as the dependent variable lm(y ~ x) Call: lm(formula = y ~ x) Coefficients: (Intercept) x 2.1348 0.7517 We can add the best-fit line to the scatter plot using the following lines: # creates short hand variable names for the columns. x = log(mammals$body) y = log(mammals$brain) # draws the scatter plot of x versus y. plot(x, y, xlab=\"log(body)\", ylab=\"log(brain)\", las=1) # abline() adds a straight line to the plot, # the line we want to add is the linear model, so we use lm(). abline(lm(y ~ x)) --- Example 2 The data set of this example comes from the package \"UsingR\". To see more about that package: UsingR First, we need to install this package. # you install a package only once install.packages(\"UsingR\") Updating HTML index of packages in '.Library' Making 'packages.html' ... done Then we need to import the package (i.e. tell R that we will use it in the session). # you import a package every time you restart rstudio library(UsingR) Loading required package: HistData Loading required package: Hmisc Loading required package: lattice Loading required package: survival Loading required package: Formula Loading required package: ggplot2 Attaching package: \u2018Hmisc\u2019 The following objects are masked from \u2018package:base\u2019: format.pval, units Attaching package: \u2018UsingR\u2019 The following object is masked from \u2018package:survival\u2019: cancer twins is a data frame that comes with that package. str(twins) 'data.frame': 27 obs. of 3 variables: $ Foster : num 82 80 88 108 116 117 132 71 75 93 ... $ Biological: num 82 90 91 115 115 129 131 78 79 82 ... $ Social : Factor w/ 3 levels \"high\",\"low\",\"middle\": 1 1 1 1 1 1 1 3 3 3 ... head(twins) A data.frame: 6 \u00d7 3 Foster Biological Social <dbl> <dbl> <fct> 1 82 82 high 2 80 90 high 3 88 91 high 4 108 115 high 5 116 115 high 6 117 129 high summary(twins) Foster Biological Social Min. : 63.00 Min. : 68.0 high : 7 1st Qu.: 84.50 1st Qu.: 83.5 low :14 Median : 94.00 Median : 94.0 middle: 6 Mean : 95.11 Mean : 95.3 3rd Qu.:107.50 3rd Qu.:104.5 Max. :132.00 Max. :131.0 Actually, this is quite a controversial data set https://www.intelltheory.com/burtaffair.shtml The data set has 3 variables (3 columns). 27 twins are studied. They were seperated at birth. One kid is raised in a foster family. One The Foster column contains the IQ of the child whoraised in the foster family, Biological column contains the IQ level of the child that is raised in his/her biological family. Social is a categorical variable that tells about the social status of the biological parents (high, middle or low). An interesting data visualization is the difference between the IQ levels of the identical twins (one raised in a foster family and the other one is raised in the biological family). boxplot(twins$Foster - twins$Biological) This boxplot looks symmetric. But a more interesting visualization would be that IQ difference versus the social status of the biological parents. The ~ in the (Foster - Biological) ~ Social stands for versus . The second variable is taken to be the independent variable (x-axis) and the first variable is taken to be the dependent variable (y-axis). Since the Social is a categorical variable, the values for it are discrete: low , middle high . Since we'll spend some time with twins data, we may as well attach the data frame. attach(twins) # takes the twins dataset # takes Social as the horizontal variable # takes Foster - Biological as the vertical variable # draws boxplots for different values of the horizontal variable \"Social\" boxplot((Foster-Biological) ~ Social, twins) Similarly, we can draw a scatterplot without detailing by the Social values: plot(twins$Foster, twins$Biological) Again, we achieve a better visualization if we take the values of the Social into account. To make a nice looking scatter plot, we first will one-hot encode the Social column: In other words, we will convert the text values in the Social column to numbers. status = as.integer(Social) status .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 1 1 1 1 1 1 1 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 Social .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} high high high high high high high middle middle middle middle middle middle low low low low low low low low low low low low low low Levels : .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 'high' 'low' 'middle' The value high is converted to 1, middle to 3, and low to 2 in the status variable. # create a scatter plot: # pch argument takes numerical values (which is why we defined the status variable above) # and determines the point shapes: triangle, circle etc. # # col determines the color. plot(Foster ~ Biological, data=twins, pch=status, col=Social) # add a legend to topleft corner. legend(\"topleft\", c(\"high\",\"low\",\"middle\"), pch=1:3, col=1:3, inset=.02) # add a linear model for the aggregate data. abline(lm(Foster ~ Biological)) To learn more about what you can do with the plot() function, you can check out this website: Basic and Advanced Graphics in R . We can also draw conditional scatter plots. You can think of a specifying a condition as filtering/masking for each value of the condition variable. In the visualization below, three scatterplots are produced. One for each value of the variable Social : low , middle , high . coplot(Foster ~ Biological | Social, data = twins, subscripts = TRUE) Warning message in plot.xy(xy.coords(x, y), type = type, ...): \u201c\"subscripts\" is not a graphical parameter\u201d Warning message in plot.xy(xy.coords(x, y), type = type, ...): \u201c\"subscripts\" is not a graphical parameter\u201d Warning message in plot.xy(xy.coords(x, y), type = type, ...): \u201c\"subscripts\" is not a graphical parameter\u201d One problem with this coplot is that the order is not clear: high is lower left, low is lower right, middle is top left (since high, low and middle is how the values are ordered alphabetically). For a better looking graph, we can use the xyplot() function which comes with the lattice package. xyplot(Foster ~ Biological|Social, data=twins) As usual, we can change the point shapes and the colors using the pch and col arguments. xyplot(Foster ~ Biological|Social, data=twins, pch=20, col=1) If you want what else you can do with the lattice package, take a look at this website: Getting Started with Lattice Graphics . Once you're done working with an attached data frame, don't forget to detach it. detach(twins) Multivariate Data: Several Quantitative Variables Example 3 You can import the data as follows: By the way, the link in the textbook didn't work. I took the data from: Brain Size and Intelligence . Column name Description Gender Male or Female FSIQ Full Scale IQ scores based on the four Wechsler (1981) subtests VIQ Verbal IQ scores based on the four Wechsler (1981) subtests PIQ Performance IQ scores based on the four Wechsler (1981) subtests Weight body weight in pounds Height height in inches MRI Count total pixel count from the 18 MRI scans brain = read.csv(\"http://bcs.whfreeman.com/WebPub/Statistics/shared_resources/EESEE/BrainSize/Data_Files/BRAINSZE.TXT\", sep = '\\t', header = TRUE) head(brain) A data.frame: 6 \u00d7 7 Gender FSIQ VIQ PIQ Weight Height MRICount <chr> <int> <int> <int> <chr> <chr> <int> 1 Female 133 132 124 118 64.5 816932 2 Male 140 150 124 . 72.5 1001121 3 Male 139 123 150 143 73.3 1038437 4 Male 133 129 128 172 68.8 965353 5 Female 137 132 134 147 65 951545 6 Female 99 90 110 146 69 928799 class(brain$Gender) 'character' summary(brain) Gender FSIQ VIQ PIQ Length:40 Min. : 77.00 Min. : 71.0 Min. : 72.00 Class :character 1st Qu.: 89.75 1st Qu.: 90.0 1st Qu.: 88.25 Mode :character Median :116.50 Median :113.0 Median :115.00 Mean :113.45 Mean :112.3 Mean :111.03 3rd Qu.:135.50 3rd Qu.:129.8 3rd Qu.:128.00 Max. :144.00 Max. :150.0 Max. :150.00 Weight Height MRICount Length:40 Length:40 Min. : 790619 Class :character Class :character 1st Qu.: 855918 Mode :character Mode :character Median : 905399 Mean : 908755 3rd Qu.: 950078 Max. :1079549 We have a little issue here: The Gender , Weight and Height columns are not summarized nicely: The summary of the Gender category should be a frequency table. And Weight and Height are numerical variables, but we don't have the descriptive statistics of those variables unlike the other numerical categories. class(brain$Gender) 'character' class(brain$Weight) 'character' The reason is that those variables are stored as character . # change the data type of the Gender column. brain$Gender = as.factor(brain$Gender) summary(brain) Gender FSIQ VIQ PIQ Female:20 Min. : 77.00 Min. : 71.0 Min. : 72.00 Male :20 1st Qu.: 89.75 1st Qu.: 90.0 1st Qu.: 88.25 Median :116.50 Median :113.0 Median :115.00 Mean :113.45 Mean :112.3 Mean :111.03 3rd Qu.:135.50 3rd Qu.:129.8 3rd Qu.:128.00 Max. :144.00 Max. :150.0 Max. :150.00 Weight Height MRICount Length:40 Length:40 Min. : 790619 Class :character Class :character 1st Qu.: 855918 Mode :character Mode :character Median : 905399 Mean : 908755 3rd Qu.: 950078 Max. :1079549 The Gender column is fixed. To see what was wrong with the Weight and Height columns, let's look at the values in those columns: brain$Weight .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} '118' '.' '143' '172' '147' '146' '138' '175' '134' '172' '118' '151' '155' '155' '146' '135' '127' '178' '136' '180' '.' '186' '122' '132' '114' '171' '140' '187' '106' '159' '127' '191' '192' '181' '143' '153' '144' '139' '148' '179' Dots were used for missing data, and R ended up interpreting the values as text instead of numerical. brain$Weight = as.numeric(brain$Weight) brain$Height = as.numeric(brain$Height) Warning message in eval(expr, envir, enclos): \u201cNAs introduced by coercion\u201d Warning message in eval(expr, envir, enclos): \u201cNAs introduced by coercion\u201d brain$Weight .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 118 <NA> 143 172 147 146 138 175 134 172 118 151 155 155 146 135 127 178 136 180 <NA> 186 122 132 114 171 140 187 106 159 127 191 192 181 143 153 144 139 148 179 Note that the dots are converted to NA (which is R way of saying a numeric value is not available.) summary(brain) Gender FSIQ VIQ PIQ Weight Female:20 Min. : 77.00 Min. : 71.0 Min. : 72.00 Min. :106.0 Male :20 1st Qu.: 89.75 1st Qu.: 90.0 1st Qu.: 88.25 1st Qu.:135.2 Median :116.50 Median :113.0 Median :115.00 Median :146.5 Mean :113.45 Mean :112.3 Mean :111.03 Mean :151.1 3rd Qu.:135.50 3rd Qu.:129.8 3rd Qu.:128.00 3rd Qu.:172.0 Max. :144.00 Max. :150.0 Max. :150.00 Max. :192.0 NA's :2 Height MRICount Min. :62.00 Min. : 790619 1st Qu.:66.00 1st Qu.: 855918 Median :68.00 Median : 905399 Mean :68.53 Mean : 908755 3rd Qu.:70.50 3rd Qu.: 950078 Max. :77.00 Max. :1079549 NA's :1 When you use the summary() function, R will tell you how many missing values there are and ignore those values when computing the descriptive statistics. Maybe it's not a bad idea to attach the data frame at this point. attach(brain) Our first remark is that R will have trouble when computing the mean when there are NA values in data: mean(Height) <NA> To fix that (actually this is what R does when summarizing), we can tell R to ignore those values: # remove the NA values and then compute the mean. mean(Height, na.rm = TRUE) 68.525641025641 One obvious grouping in this data set is by Gender . If we want display statistics seperately for males and females, we can use the by() function. by(data=brain, INDICES=brain$Gender, FUN=summary, na.rm=TRUE) # INDICES: where to look when grouping. # FUN: (short for function), what to compute (we want summary in this example) # na.rm = TRUE: same as above, remove the na values when computing. brain$Gender: Female Gender FSIQ VIQ PIQ Weight Female:20 Min. : 77.00 Min. : 71.0 Min. : 72.0 Min. :106.0 Male : 0 1st Qu.: 90.25 1st Qu.: 90.0 1st Qu.: 93.0 1st Qu.:125.8 Median :115.50 Median :116.0 Median :115.0 Median :138.5 Mean :111.90 Mean :109.5 Mean :110.5 Mean :137.2 3rd Qu.:133.00 3rd Qu.:129.0 3rd Qu.:128.8 3rd Qu.:146.2 Max. :140.00 Max. :136.0 Max. :147.0 Max. :175.0 Height MRICount Min. :62.00 Min. :790619 1st Qu.:64.50 1st Qu.:828062 Median :66.00 Median :855365 Mean :65.77 Mean :862655 3rd Qu.:66.88 3rd Qu.:882668 Max. :70.50 Max. :991305 ------------------------------------------------------------ brain$Gender: Male Gender FSIQ VIQ PIQ Weight Female: 0 Min. : 80.00 Min. : 77.00 Min. : 74.0 Min. :132.0 Male :20 1st Qu.: 89.75 1st Qu.: 95.25 1st Qu.: 86.0 1st Qu.:148.8 Median :118.00 Median :110.50 Median :117.0 Median :172.0 Mean :115.00 Mean :115.25 Mean :111.6 Mean :166.4 3rd Qu.:139.25 3rd Qu.:145.00 3rd Qu.:128.0 3rd Qu.:180.8 Max. :144.00 Max. :150.00 Max. :150.0 Max. :192.0 NA's :2 Height MRICount Min. :66.30 Min. : 879987 1st Qu.:68.90 1st Qu.: 919529 Median :70.50 Median : 947242 Mean :71.43 Mean : 954855 3rd Qu.:73.75 3rd Qu.: 973496 Max. :77.00 Max. :1079549 NA's :1 Let's make some plots. # first, convert the Gender values to numbers # to be able to use them as symbol or color gender = as.integer(Gender) plot(Weight, MRICount, pch=gender, col=gender) legend(\"topleft\", c(\"Female\", \"Male\"), pch=1:2, col=1:2, inset=.02) One cool visualization is pairing the numerical values and plotting all possible scatterplots at once: # note that the first column is omitted. pairs(brain[, 2:7]) We can quickly compute the correlation coefficients between pairs of variables by applying a single cor() to the whole data frame (except the first column, which is not a numerical variable). cor(brain[,2:7]) A matrix: 6 \u00d7 6 of type dbl FSIQ VIQ PIQ Weight Height MRICount FSIQ 1.0000000 0.9466388 0.9341251 NA NA 0.3576410 VIQ 0.9466388 1.0000000 0.7781351 NA NA 0.3374777 PIQ 0.9341251 0.7781351 1.0000000 NA NA 0.3868173 Weight NA NA NA 1 NA NA Height NA NA NA NA 1 NA MRICount 0.3576410 0.3374777 0.3868173 NA NA 1.0000000 There are two issues with the table above: 1. Big issue: Some of the correlation coefficients were not computed (due to missing values). 2. Minor issue: There are simply to many decimals which doesn't look nice. # round(,2) rounds the number in the first argument to 2 decimals. # use=\"pairwise.complete.obs\" will only use complete pairs, # i.e. ignores the pairs if one of the values is NA. round(cor(brain[, 2:7], use=\"pairwise.complete.obs\"), 2) A matrix: 6 \u00d7 6 of type dbl FSIQ VIQ PIQ Weight Height MRICount FSIQ 1.00 0.95 0.93 -0.05 -0.09 0.36 VIQ 0.95 1.00 0.78 -0.08 -0.07 0.34 PIQ 0.93 0.78 1.00 0.00 -0.08 0.39 Weight -0.05 -0.08 0.00 1.00 0.70 0.51 Height -0.09 -0.07 -0.08 0.70 1.00 0.60 MRICount 0.36 0.34 0.39 0.51 0.60 1.00 From this table, we can see that there is a strong correlation between each of the IQ scores, where as the height or weight don't seem to be correlated to IQ at all.","title":"02 - Chapter 2"},{"location":"chapter_2/quantitative/#chapter-2-quantitive-data","text":"","title":"Chapter 2: Quantitive Data"},{"location":"chapter_2/quantitative/#bivariate-data-two-quantitive-numerical-variables","text":"We start with importing a data set library called MASS . It contains the example we will be working with. # import the MASS data set library. library(MASS) This import will enlarge the data set examples in R . To see a list of new data sets that come with the MASS package, you can simply the following command. # list the data sets that come with MASS data(package = 'MASS') Or, simply list of all data sets currently accessable in your session. (The data sets will be sorted in blocks according the package they come with.) # list all data sets in `R` (the updated list) data()","title":"Bivariate Data: Two Quantitive (Numerical) Variables"},{"location":"chapter_2/quantitative/#example-1","text":"In this example, we look at the mammals data set. # see how the data frame looks like by inspecting the first few rows. head(mammals) A data.frame: 6 \u00d7 2 body brain <dbl> <dbl> Arctic fox 3.385 44.5 Owl monkey 0.480 15.5 Mountain beaver 1.350 8.1 Cow 465.000 423.0 Grey wolf 36.330 119.5 Goat 27.660 115.0 # how many rows and columns the data set contains. dim(mammals) .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 62 2 After looking at the first few rows, and the dimension of the data frame, we see that the data set mammals contains body and brain sizes of 62 different mammal species. If you want to learn more about the data set mammals , you can check the documentation of the data set by simply typing ?mammals or help(mammals) . # if you want to learn more about the data set, you can check the documentation ?mammals To make sure the format of the data set you can type the following commands: # is the data set stored in a matrix? is.matrix(mammals) FALSE # is the data set stored in a data frame? is.data.frame(mammals) TRUE # or just ask for the class: data frame or matrix? class(mammals) 'data.frame' # to see the answers of most of the questions above, you can simply type str(mammals) 'data.frame': 62 obs. of 2 variables: $ body : num 3.38 0.48 1.35 465 36.33 ... $ brain: num 44.5 15.5 8.1 423 119.5 ... The command summary() summarizes each numerical column of the data frame and gives the frequency distributions of the categorical columns. summary(mammals) body brain Min. : 0.005 Min. : 0.14 1st Qu.: 0.600 1st Qu.: 4.25 Median : 3.342 Median : 17.25 Mean : 198.790 Mean : 283.13 3rd Qu.: 48.202 3rd Qu.: 166.00 Max. :6654.000 Max. :5712.00 The problem with this data set is that the difference between the max and min is huge!","title":"Example 1"},{"location":"chapter_2/quantitative/#digression","text":"What are the smallest and largest animals in the mammals dataset? # which.min() gives the row number that contains the minimum of the column you specify # then to display that row, you can use the [] method mammals[which.min(mammals$body), ] A data.frame: 1 \u00d7 2 body brain <dbl> <dbl> Lesser short-tailed shrew 0.005 0.14 mammals[which.max(mammals$body), ] A data.frame: 1 \u00d7 2 body brain <dbl> <dbl> African elephant 6654 5712","title":"Digression"},{"location":"chapter_2/quantitative/#back-to-the-example","text":"To see a visualization of the 5-number-summary (min, Q1, median, Q2, max), we use boxplots. # to see the boxplot of each numerical column boxplot(mammals) The problem with this visualization is that the most of the mammals are relatively small (compared to a few huge ones), and to fit the bigger mammals into the graph, the boxplots are squeezed down all the way down. You can easy create scatter plots for every pair of variables (our dataset contains two variables body and brain hence there will be only one scatter plot). plot(mammals) Again, most of the points are squeezed down to the bottom left corner due to the existence of the very large mammals such as the elephant. What our textbook suggest is to using the logarithmic scale and see the order of magnitudes of the numbers. For those of you who don't remember much about logarithms:","title":"Back to the example:"},{"location":"chapter_2/quantitative/#digression-logarithms","text":"The (natural) logarithm of a (positive) number $b$ is the power you need to raise the Euler number $e$ (which is an irrational number like $\\pi$ that happens to have very nice properties in math) to get $b$: $$\\log(a)= b \\leftrightarrow e^b = a$$ The Euler number $e$ has the approximate decimal value $2.7182$. To display $e$ in R : exp(1) What logarithm computes: log(1000) (exp(1))^6.907755279 # the error is due to rounding. $$\\log(1000)\\approx 6.908 \\leftrightarrow e^{6.908} \\approx 1000$$ You can think of $\\log(1000)$ as the number that tells how big 1000 is multiplicatively (when the standard/unit is taken to be the number $e\\approx 2.7182$.)","title":"Digression: Logarithms"},{"location":"chapter_2/quantitative/#back-to-the-example_1","text":"summary(log(mammals)) These numbers tell you how big the values are multiplicatively. This is a neat trick: If the numbers deviate a lot additively (i.e. max - min is too big), you may want to compare the numbers multiplicatively. plot(log(mammals), xlab=\"log(body)\", ylab=\"log(brain)\", las=1) boxplot(log(mammals)) Note that when you think of the numeric data in a multiplicative way, the visualizations look much better. One very important number that summarizes the relationship between two numberical variables is the (Pearson) correlation coefficient. It measures how strongly the values depend on each other: The (Pearson) correlation coefficient $r$ tells us two important things: The sign of the correlation coefficient between the variables tells us if the relation is positive or negative, i.e. if the second variable increases or decreases on average, when the first variable increases. The size of the correlation coefficient tells how strongly the variables are related. If the $|r|$ is close to 1, then the variables are strongly (linearly) related, and if $|r|$ is close to 0, then the strength of the relation between the variables is quite weak, if we can say there is a relation at all. cor(log(mammals)) A matrix: 2 \u00d7 2 of type dbl body brain body 1.0000000 0.9595748 brain 0.9595748 1.0000000 Staying in the log-log scale, we see that brain and body variables have a correlation constant of 0.960, which is pretty close to 1. In other words, we can pretty accurately say, the larger the body size, the larger the brain size for mammals on average. (I know this sounds as a trivial observation, but think of the dinosaurs: dinosaur brain .) If there is a strong linear relation between two numeric variables (which we can find out by looking at the correlation coefficient of them), we can model that relation using a line (the simplest non-trivial model). # to simplify the prompts, first give log() values new names x = log(mammals$body) y = log(mammals$brain) # gives the best fitting line #when x is taken as the independent variable and y as the dependent variable lm(y ~ x) Call: lm(formula = y ~ x) Coefficients: (Intercept) x 2.1348 0.7517 We can add the best-fit line to the scatter plot using the following lines: # creates short hand variable names for the columns. x = log(mammals$body) y = log(mammals$brain) # draws the scatter plot of x versus y. plot(x, y, xlab=\"log(body)\", ylab=\"log(brain)\", las=1) # abline() adds a straight line to the plot, # the line we want to add is the linear model, so we use lm(). abline(lm(y ~ x))","title":"Back to the example"},{"location":"chapter_2/quantitative/#-","text":"","title":"---"},{"location":"chapter_2/quantitative/#example-2","text":"The data set of this example comes from the package \"UsingR\". To see more about that package: UsingR First, we need to install this package. # you install a package only once install.packages(\"UsingR\") Updating HTML index of packages in '.Library' Making 'packages.html' ... done Then we need to import the package (i.e. tell R that we will use it in the session). # you import a package every time you restart rstudio library(UsingR) Loading required package: HistData Loading required package: Hmisc Loading required package: lattice Loading required package: survival Loading required package: Formula Loading required package: ggplot2 Attaching package: \u2018Hmisc\u2019 The following objects are masked from \u2018package:base\u2019: format.pval, units Attaching package: \u2018UsingR\u2019 The following object is masked from \u2018package:survival\u2019: cancer twins is a data frame that comes with that package. str(twins) 'data.frame': 27 obs. of 3 variables: $ Foster : num 82 80 88 108 116 117 132 71 75 93 ... $ Biological: num 82 90 91 115 115 129 131 78 79 82 ... $ Social : Factor w/ 3 levels \"high\",\"low\",\"middle\": 1 1 1 1 1 1 1 3 3 3 ... head(twins) A data.frame: 6 \u00d7 3 Foster Biological Social <dbl> <dbl> <fct> 1 82 82 high 2 80 90 high 3 88 91 high 4 108 115 high 5 116 115 high 6 117 129 high summary(twins) Foster Biological Social Min. : 63.00 Min. : 68.0 high : 7 1st Qu.: 84.50 1st Qu.: 83.5 low :14 Median : 94.00 Median : 94.0 middle: 6 Mean : 95.11 Mean : 95.3 3rd Qu.:107.50 3rd Qu.:104.5 Max. :132.00 Max. :131.0 Actually, this is quite a controversial data set https://www.intelltheory.com/burtaffair.shtml The data set has 3 variables (3 columns). 27 twins are studied. They were seperated at birth. One kid is raised in a foster family. One The Foster column contains the IQ of the child whoraised in the foster family, Biological column contains the IQ level of the child that is raised in his/her biological family. Social is a categorical variable that tells about the social status of the biological parents (high, middle or low). An interesting data visualization is the difference between the IQ levels of the identical twins (one raised in a foster family and the other one is raised in the biological family). boxplot(twins$Foster - twins$Biological) This boxplot looks symmetric. But a more interesting visualization would be that IQ difference versus the social status of the biological parents. The ~ in the (Foster - Biological) ~ Social stands for versus . The second variable is taken to be the independent variable (x-axis) and the first variable is taken to be the dependent variable (y-axis). Since the Social is a categorical variable, the values for it are discrete: low , middle high . Since we'll spend some time with twins data, we may as well attach the data frame. attach(twins) # takes the twins dataset # takes Social as the horizontal variable # takes Foster - Biological as the vertical variable # draws boxplots for different values of the horizontal variable \"Social\" boxplot((Foster-Biological) ~ Social, twins) Similarly, we can draw a scatterplot without detailing by the Social values: plot(twins$Foster, twins$Biological) Again, we achieve a better visualization if we take the values of the Social into account. To make a nice looking scatter plot, we first will one-hot encode the Social column: In other words, we will convert the text values in the Social column to numbers. status = as.integer(Social) status .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 1 1 1 1 1 1 1 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 Social .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} high high high high high high high middle middle middle middle middle middle low low low low low low low low low low low low low low Levels : .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 'high' 'low' 'middle' The value high is converted to 1, middle to 3, and low to 2 in the status variable. # create a scatter plot: # pch argument takes numerical values (which is why we defined the status variable above) # and determines the point shapes: triangle, circle etc. # # col determines the color. plot(Foster ~ Biological, data=twins, pch=status, col=Social) # add a legend to topleft corner. legend(\"topleft\", c(\"high\",\"low\",\"middle\"), pch=1:3, col=1:3, inset=.02) # add a linear model for the aggregate data. abline(lm(Foster ~ Biological)) To learn more about what you can do with the plot() function, you can check out this website: Basic and Advanced Graphics in R . We can also draw conditional scatter plots. You can think of a specifying a condition as filtering/masking for each value of the condition variable. In the visualization below, three scatterplots are produced. One for each value of the variable Social : low , middle , high . coplot(Foster ~ Biological | Social, data = twins, subscripts = TRUE) Warning message in plot.xy(xy.coords(x, y), type = type, ...): \u201c\"subscripts\" is not a graphical parameter\u201d Warning message in plot.xy(xy.coords(x, y), type = type, ...): \u201c\"subscripts\" is not a graphical parameter\u201d Warning message in plot.xy(xy.coords(x, y), type = type, ...): \u201c\"subscripts\" is not a graphical parameter\u201d One problem with this coplot is that the order is not clear: high is lower left, low is lower right, middle is top left (since high, low and middle is how the values are ordered alphabetically). For a better looking graph, we can use the xyplot() function which comes with the lattice package. xyplot(Foster ~ Biological|Social, data=twins) As usual, we can change the point shapes and the colors using the pch and col arguments. xyplot(Foster ~ Biological|Social, data=twins, pch=20, col=1) If you want what else you can do with the lattice package, take a look at this website: Getting Started with Lattice Graphics . Once you're done working with an attached data frame, don't forget to detach it. detach(twins)","title":"Example 2"},{"location":"chapter_2/quantitative/#multivariate-data-several-quantitative-variables","text":"","title":"Multivariate Data: Several Quantitative Variables"},{"location":"chapter_2/quantitative/#example-3","text":"You can import the data as follows: By the way, the link in the textbook didn't work. I took the data from: Brain Size and Intelligence . Column name Description Gender Male or Female FSIQ Full Scale IQ scores based on the four Wechsler (1981) subtests VIQ Verbal IQ scores based on the four Wechsler (1981) subtests PIQ Performance IQ scores based on the four Wechsler (1981) subtests Weight body weight in pounds Height height in inches MRI Count total pixel count from the 18 MRI scans brain = read.csv(\"http://bcs.whfreeman.com/WebPub/Statistics/shared_resources/EESEE/BrainSize/Data_Files/BRAINSZE.TXT\", sep = '\\t', header = TRUE) head(brain) A data.frame: 6 \u00d7 7 Gender FSIQ VIQ PIQ Weight Height MRICount <chr> <int> <int> <int> <chr> <chr> <int> 1 Female 133 132 124 118 64.5 816932 2 Male 140 150 124 . 72.5 1001121 3 Male 139 123 150 143 73.3 1038437 4 Male 133 129 128 172 68.8 965353 5 Female 137 132 134 147 65 951545 6 Female 99 90 110 146 69 928799 class(brain$Gender) 'character' summary(brain) Gender FSIQ VIQ PIQ Length:40 Min. : 77.00 Min. : 71.0 Min. : 72.00 Class :character 1st Qu.: 89.75 1st Qu.: 90.0 1st Qu.: 88.25 Mode :character Median :116.50 Median :113.0 Median :115.00 Mean :113.45 Mean :112.3 Mean :111.03 3rd Qu.:135.50 3rd Qu.:129.8 3rd Qu.:128.00 Max. :144.00 Max. :150.0 Max. :150.00 Weight Height MRICount Length:40 Length:40 Min. : 790619 Class :character Class :character 1st Qu.: 855918 Mode :character Mode :character Median : 905399 Mean : 908755 3rd Qu.: 950078 Max. :1079549 We have a little issue here: The Gender , Weight and Height columns are not summarized nicely: The summary of the Gender category should be a frequency table. And Weight and Height are numerical variables, but we don't have the descriptive statistics of those variables unlike the other numerical categories. class(brain$Gender) 'character' class(brain$Weight) 'character' The reason is that those variables are stored as character . # change the data type of the Gender column. brain$Gender = as.factor(brain$Gender) summary(brain) Gender FSIQ VIQ PIQ Female:20 Min. : 77.00 Min. : 71.0 Min. : 72.00 Male :20 1st Qu.: 89.75 1st Qu.: 90.0 1st Qu.: 88.25 Median :116.50 Median :113.0 Median :115.00 Mean :113.45 Mean :112.3 Mean :111.03 3rd Qu.:135.50 3rd Qu.:129.8 3rd Qu.:128.00 Max. :144.00 Max. :150.0 Max. :150.00 Weight Height MRICount Length:40 Length:40 Min. : 790619 Class :character Class :character 1st Qu.: 855918 Mode :character Mode :character Median : 905399 Mean : 908755 3rd Qu.: 950078 Max. :1079549 The Gender column is fixed. To see what was wrong with the Weight and Height columns, let's look at the values in those columns: brain$Weight .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} '118' '.' '143' '172' '147' '146' '138' '175' '134' '172' '118' '151' '155' '155' '146' '135' '127' '178' '136' '180' '.' '186' '122' '132' '114' '171' '140' '187' '106' '159' '127' '191' '192' '181' '143' '153' '144' '139' '148' '179' Dots were used for missing data, and R ended up interpreting the values as text instead of numerical. brain$Weight = as.numeric(brain$Weight) brain$Height = as.numeric(brain$Height) Warning message in eval(expr, envir, enclos): \u201cNAs introduced by coercion\u201d Warning message in eval(expr, envir, enclos): \u201cNAs introduced by coercion\u201d brain$Weight .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 118 <NA> 143 172 147 146 138 175 134 172 118 151 155 155 146 135 127 178 136 180 <NA> 186 122 132 114 171 140 187 106 159 127 191 192 181 143 153 144 139 148 179 Note that the dots are converted to NA (which is R way of saying a numeric value is not available.) summary(brain) Gender FSIQ VIQ PIQ Weight Female:20 Min. : 77.00 Min. : 71.0 Min. : 72.00 Min. :106.0 Male :20 1st Qu.: 89.75 1st Qu.: 90.0 1st Qu.: 88.25 1st Qu.:135.2 Median :116.50 Median :113.0 Median :115.00 Median :146.5 Mean :113.45 Mean :112.3 Mean :111.03 Mean :151.1 3rd Qu.:135.50 3rd Qu.:129.8 3rd Qu.:128.00 3rd Qu.:172.0 Max. :144.00 Max. :150.0 Max. :150.00 Max. :192.0 NA's :2 Height MRICount Min. :62.00 Min. : 790619 1st Qu.:66.00 1st Qu.: 855918 Median :68.00 Median : 905399 Mean :68.53 Mean : 908755 3rd Qu.:70.50 3rd Qu.: 950078 Max. :77.00 Max. :1079549 NA's :1 When you use the summary() function, R will tell you how many missing values there are and ignore those values when computing the descriptive statistics. Maybe it's not a bad idea to attach the data frame at this point. attach(brain) Our first remark is that R will have trouble when computing the mean when there are NA values in data: mean(Height) <NA> To fix that (actually this is what R does when summarizing), we can tell R to ignore those values: # remove the NA values and then compute the mean. mean(Height, na.rm = TRUE) 68.525641025641 One obvious grouping in this data set is by Gender . If we want display statistics seperately for males and females, we can use the by() function. by(data=brain, INDICES=brain$Gender, FUN=summary, na.rm=TRUE) # INDICES: where to look when grouping. # FUN: (short for function), what to compute (we want summary in this example) # na.rm = TRUE: same as above, remove the na values when computing. brain$Gender: Female Gender FSIQ VIQ PIQ Weight Female:20 Min. : 77.00 Min. : 71.0 Min. : 72.0 Min. :106.0 Male : 0 1st Qu.: 90.25 1st Qu.: 90.0 1st Qu.: 93.0 1st Qu.:125.8 Median :115.50 Median :116.0 Median :115.0 Median :138.5 Mean :111.90 Mean :109.5 Mean :110.5 Mean :137.2 3rd Qu.:133.00 3rd Qu.:129.0 3rd Qu.:128.8 3rd Qu.:146.2 Max. :140.00 Max. :136.0 Max. :147.0 Max. :175.0 Height MRICount Min. :62.00 Min. :790619 1st Qu.:64.50 1st Qu.:828062 Median :66.00 Median :855365 Mean :65.77 Mean :862655 3rd Qu.:66.88 3rd Qu.:882668 Max. :70.50 Max. :991305 ------------------------------------------------------------ brain$Gender: Male Gender FSIQ VIQ PIQ Weight Female: 0 Min. : 80.00 Min. : 77.00 Min. : 74.0 Min. :132.0 Male :20 1st Qu.: 89.75 1st Qu.: 95.25 1st Qu.: 86.0 1st Qu.:148.8 Median :118.00 Median :110.50 Median :117.0 Median :172.0 Mean :115.00 Mean :115.25 Mean :111.6 Mean :166.4 3rd Qu.:139.25 3rd Qu.:145.00 3rd Qu.:128.0 3rd Qu.:180.8 Max. :144.00 Max. :150.00 Max. :150.0 Max. :192.0 NA's :2 Height MRICount Min. :66.30 Min. : 879987 1st Qu.:68.90 1st Qu.: 919529 Median :70.50 Median : 947242 Mean :71.43 Mean : 954855 3rd Qu.:73.75 3rd Qu.: 973496 Max. :77.00 Max. :1079549 NA's :1 Let's make some plots. # first, convert the Gender values to numbers # to be able to use them as symbol or color gender = as.integer(Gender) plot(Weight, MRICount, pch=gender, col=gender) legend(\"topleft\", c(\"Female\", \"Male\"), pch=1:2, col=1:2, inset=.02) One cool visualization is pairing the numerical values and plotting all possible scatterplots at once: # note that the first column is omitted. pairs(brain[, 2:7]) We can quickly compute the correlation coefficients between pairs of variables by applying a single cor() to the whole data frame (except the first column, which is not a numerical variable). cor(brain[,2:7]) A matrix: 6 \u00d7 6 of type dbl FSIQ VIQ PIQ Weight Height MRICount FSIQ 1.0000000 0.9466388 0.9341251 NA NA 0.3576410 VIQ 0.9466388 1.0000000 0.7781351 NA NA 0.3374777 PIQ 0.9341251 0.7781351 1.0000000 NA NA 0.3868173 Weight NA NA NA 1 NA NA Height NA NA NA NA 1 NA MRICount 0.3576410 0.3374777 0.3868173 NA NA 1.0000000 There are two issues with the table above: 1. Big issue: Some of the correlation coefficients were not computed (due to missing values). 2. Minor issue: There are simply to many decimals which doesn't look nice. # round(,2) rounds the number in the first argument to 2 decimals. # use=\"pairwise.complete.obs\" will only use complete pairs, # i.e. ignores the pairs if one of the values is NA. round(cor(brain[, 2:7], use=\"pairwise.complete.obs\"), 2) A matrix: 6 \u00d7 6 of type dbl FSIQ VIQ PIQ Weight Height MRICount FSIQ 1.00 0.95 0.93 -0.05 -0.09 0.36 VIQ 0.95 1.00 0.78 -0.08 -0.07 0.34 PIQ 0.93 0.78 1.00 0.00 -0.08 0.39 Weight -0.05 -0.08 0.00 1.00 0.70 0.51 Height -0.09 -0.07 -0.08 0.70 1.00 0.60 MRICount 0.36 0.34 0.39 0.51 0.60 1.00 From this table, we can see that there is a strong correlation between each of the IQ scores, where as the height or weight don't seem to be correlated to IQ at all.","title":"Example 3"},{"location":"chapter_6/basic_inference/","text":"Chapter 6: Basic Inference Methods Learning About a Proportion A proportion in statistics is a fraction of the population ( population proportion ) or sample ( sample proportion ) that has a certain characteristic. When we say the proportion of iPhones in the mobile phone market is 20%, then that means 20% of the mobile phones are iPhones and equivalently, we can also say that if you pick a random mobile phone, there is 20% probability that it is an iPhone. Similarly, if there are 20 students in your classroom and 2 of them are failing your class, you can say that the sample proportion of passing the class is 90%. Example 1: Sleeping patterns of college students Question: Is the median hours of sleep for the population of all students who take a particular class 9 hours? Sample data: Sleeping times of 24 students who take that class. Sleeping times of students 7.75, 8.5, 8, 6, 8, 6.33, 8.17, 7.75, 7, 6.5, 8.75, 8, 7.5, 3, 6.25, 8.5, 9, 6.5, 9, 9.5, 9, 8, 8, 9.5 We will do two smart things: 1. Instead of asking if the median is 9 hours, we can equivalently ask if the proportion of students who sleep more than 9 hours is 50%. 2. We will formulate our question in form of a hypothesis test. Hypothesis Tests: A hypothesis is a claim about the value of a population characteristic. A hypothesis test is like a courtroom scene: Courtroom Hypothesis Test Initial assumption: The defendent is innocent. (It's the duty of the prosecutor to prove the defendent is guilty.) The null hypothesis. (We assume some value for the population characteristic.) The prosecutor tries to prove the defendant is guilty. Alternative hypothesis. (We want to know if our sample statistic provides sufficiently strong evidence in some desired direction away from the null hypothesis value.) Evidence. Sample statistic(s). Strong evidence. Sample statistic is far (in the desired direction.) Decision: Either the defendent is guilty or is not guilty. (There is no such decision stating the defendant is innocent.) Conclusion of the test: Either the null hypothesis is rejected or the null hypothesis is failed to be rejected. One important remark is that in a court of law, the defendant is never found innocent. The prosecutor tries to prove the defendant is guilty by bringing some evidence. That evidence is discussed. If the evidence is weak, the defendant is found not guilty. If the evidence is strong, the defendant is found guilty. Similarly, in a hypothesis test, we check if the sample statistic provides strong enough evidence for the alternate hypothesis. If the evidence is weak (sample statistic not far from the null hypothesis value), we fail to reject the null hypothesis . If the evidence is strong (sample statistic is far from the null hypothesis value in the desired direction), we reject the null hypothesis . There are two kind of errors that can happen. Courtroom Hypothesis test Term An innocent person is sentenced. We reject a true null hypothesis. Type I error A guilty person is not sentenced. We fail to reject a false null hypothesis. Type II error Back to our example. sleep = c(7.75, 8.5, 8, 6, 8, 6.33, 8.17, 7.75, 7, 6.5, 8.75, 8, 7.5, 3, 6.25, 8.5, 9, 6.5, 9, 9.5, 9, 8, 8, 9.5) nine.hours = ifelse(sleep >= 9, \"yes\", \"no\") table(nine.hours) nine.hours no yes 19 5 Our sample size is 24. $$n=24$$ The null hypothesis is that the population proportion is 50%. $$\\textrm{$H_0$: }p=0.5$$ In a hypothesis test, we're both the judge and the prosecutor. As a prosecutor, we're expected to give the details of our claim. Our claim by default is the null hypothesis is not true, but what is considered not true. There are three options we can choose from: 1. The population characteristic (in this example the population proportion) is greater than the hypothesized value (upper-tailed test): $$\\textrm{H} {\\textrm{alt}:} p > p_0$$ where $p_0$ denotes the value we use in null-hypothesis. 2. The population characteristic is less than the hypothesized value (lower-tailed test): $$\\textrm{H} {\\textrm{alt}:} p < p_0$$ 3. The population characteristic is different from the hypothesized value (two-tailed test): $$\\textrm{H}_{\\textrm{alt}:} p \\neq p_0$$ As a judge, we determine how extreme the sample proportion should be so that it counts as strong evidence. That threshold is quantified as $\\alpha$. Commonly used values of $\\alpha$ are 0.1, 0.05 and 0.01. We need to decide on the value of $\\alpha$ before running the test. $\\alpha=0.05$ means, the sample statistic is considered sufficiently strong evidence for the alternative hypothesis if the sample Since we want to know if the median hours of sleep is different from 9 hours, we will say the our alternative hypothesis is that the population proportion is greater or less than 50%: $$\\textrm{$H_\\textrm{alt}$: }p\\neq0.5$$ y = 5; n = 24 Test = prop.test(y, n, p=0.5, alternative=\"two.sided\", correct=FALSE) Test 1-sample proportions test without continuity correction data: y out of n, null probability 0.5 X-squared = 8.1667, df = 1, p-value = 0.004267 alternative hypothesis: true p is not equal to 0.5 95 percent confidence interval: 0.09244825 0.40470453 sample estimates: p 0.2083333 Based on the $P$-value we obtain (which measures how far our sample proportion is from the hypothesized population proportion), we reject the nullhypothesis. Test$p.value 0.00426672482217614 Note that our $P$ value $\\approx 0.0043$ is (much) less than $\\alpha=5\\%$. That means our sample proportion is quite extreme according to the null hypothesis. Our conclusion will be that the null hypothesis seems not to be true and we reject the null hypothesis. Based on our sample proportion $\\frac{5}{24}=\\approx 20.8\\%$, the population proportion should be in the following confidence interval with a confidence level of $95\\%$: Test$conf.int .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 0.0924482535285185 0.404704532531887 In other words, if we assume that 95% of all samples are to be considered not extreme, and our sample is one of those not extreme examples, the population proportion should be in the interval $(0.092,0.405)$. Our sample is a bit problematic: Our sample size is not that big (we don't know if the sample proportions are distributed normally). So, we can't fully rely on the assumption that $\\hat{p}$'s are distributed normally. Which is why our textbook offers some other tests (which we will not discuss in detail, just know that they exist and know where to find them and how to use them), and we will pick the most conservative result. Same test, but activating the correction argument of prop.test (Wilson) Test.adj = prop.test(y,n,p=0.5, alternative=\"two.sided\", correct=TRUE) Test.adj$p.value 0.00796348920654999 Test.adj$conf.int .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 0.0793597153947196 0.427065000253439 Using theoretical probability (Clopper-Pearson) The areas of the bars in a binomial distribution can be computed using probability. In this approach, we assume our data set approximately follows a binomial distribution. We mentioned binomial distributions last week when talking about the central limit theorem. You can read more on those distributions on this page: https://r-coder.com/binomial-distribution-r/ . Test.exact = binom.test(y,n,p=0.5) Test.exact$p.value 0.00661075115203857 Test.exact$conf.int .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 0.0713186171980549 0.421512843637253 Using Agresti-Coull test This is another popular \"small-sample\" method. We simply modify our dataset by artificially adding two successes and two failures. R doesn't have an implemented Agresti-Coull test, but we can write it as a function ourselves quite easily: agresti.interval = function(y, n, conf=0.95){ n1 = n + 4 y1 = y + 2 phat = y1 / n1 me = qnorm(1 - (1 - conf) / 2) * sqrt(phat * (1 - phat) / n1) c(phat - me, phat + me) } agresti.interval(y, n) .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 0.0896127978831776 0.410387202116822 Now that we have computed 95% confidence intervals in three different ways, we can pick the most conservative one: cnames = c(\"Wilson Score Interval\", \"Clopper-Pearson\",\"Agresti-Coull\") cfunctions = c(\"prop.test\", \"binom.test\", \"agresti.interval\") intervals = rbind(Test$conf.int, Test.exact$conf.int,agresti.interval(y, n)) data.frame(Name=cnames, Function=cfunctions,LO=intervals[ , 1], HI=intervals[ , 2]) A data.frame: 3 \u00d7 4 Name Function LO HI <chr> <chr> <dbl> <dbl> Wilson Score Interval prop.test 0.09244825 0.4047045 Clopper-Pearson binom.test 0.07131862 0.4215128 Agresti-Coull agresti.interval 0.08961280 0.4103872 The most conservative confidence interval is the longest one (in this example, the Clopper-Pearson test). If the interval lengths are very different, more thought is to be given to which interval to choose. In our example, they are quite similar. What's really important is that we go far enough to guarantee that 95% of the sample means is covered about the population mean (at most half the length of our interval away). Learning About a Mean In the previous discussion, we did two things: 1. We tested if our sample proportion $\\hat{p}$ is far from a hypothesized population proportion (hypothesis test for population proportion). 2. Where would be the population proportion based on our sample proportion (confidence interval). One-sample t statistic methods We can play the same game for the sample mean $\\bar{x}$. Remember that the sample means are also distributed normally with mean $\\mu_{\\bar{x}}$ equal to the population mean $\\mu_x$. $$\\textrm{$H_0$: }\\mu=8$$ $$n=24$$ $$\\alpha=0.1$$ Strictly speaking, we shouldn't really assume that the mean sleeping times are normally distributed, hence that $n$ is not large enough (>30). (Note that if the population data itself is normally distributed, then $n$ doesn't need to be large, the sample means would be distributed normally even when the sample size is small.) How do we check if our sample data comes from a data set that is normally distributed? One way is doing by hand: To check if our data is normally distributed, we can look at the histogram of sample data, and we can compare the quartiles of a normal distribution with the quartiles of our sample data. A sample that is chosen from a normal distribution is expected to be more or less normally distributed itself. sleep .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 7.75 8.5 8 6 8 6.33 8.17 7.75 7 6.5 8.75 8 7.5 3 6.25 8.5 9 6.5 9 9.5 9 8 8 9.5 hist(sleep,prob=TRUE) One visual way to check if the sample data seems to come from a normal distribution is checking its histogram. The histogram in this example has a problematic bar (rightmost). A more grounded way of checking if our sample data is close to a normal distribution is to break the area under the normal curve into 24 equal regions bounded by the x-axis below, normal distribution above and two consecutive horizontal cuts. Then we compare those cuts with our sample data. If those cut values are similar to our sample data, then our 24 values seem to be distributed more or less normally and we can conclude $n$ doesn't need to be large to apply the t-test. # cuts of normal distribution ppoints(sleep) .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 0.0208333333333333 0.0625 0.104166666666667 0.145833333333333 0.1875 0.229166666666667 0.270833333333333 0.3125 0.354166666666667 0.395833333333333 0.4375 0.479166666666667 0.520833333333333 0.5625 0.604166666666667 0.645833333333333 0.6875 0.729166666666667 0.770833333333333 0.8125 0.854166666666667 0.895833333333333 0.9375 0.979166666666667 qqnorm(sleep) # produces a normal probability plot of the times qqline(sleep) # overlays a line on the plot passing through the first and third quartiles For more about Q-Q plots, you can read: https://data.library.virginia.edu/understanding-q-q-plots/ Note that the point in the bottom left corner is super weird. We can detect the location of that outlier from the raw data or visually using the plot() function: plot(sleep) The sleep time of the outlier seems to be 3. So, let's find the index of that value in the sleep vector: min(sleep) 3 That value is 3 indeed. Now, let's locate the position of the value 3 in the sleep vector: which(sleep<3.5) 14 # double checking if the 14th term of the sleep vector is 3. sleep[14] 3 One way to deal with that outlier is to simply ignore it. # -14 inside the square brackets drops the term in the 14th position sleep.new = sleep[-14] Digression: How to remove values from a vector: https://www.statology.org/remove-element-from-vector-r/ Digression: Shapiro-Wilk Test There is actually a statistical test that measures normality of the population distribution based on the sample data. shapiro.test(sleep) Shapiro-Wilk normality test data: sleep W = 0.87238, p-value = 0.005858 The p-value is much less than $\\alpha=0.05$, so we cannot assume that the sample data comes from a normally distributed population. And for that reason, t-test may not be accurate. To read more about the Shapiro-Wilk test: https://www.statology.org/shapiro-wilk-test-r/ Now that the outlier is removed, we can run a t-test : t.test(sleep.new, mu=8, conf.level=0.90) One Sample t-test data: sleep.new t = -0.49862, df = 22, p-value = 0.623 alternative hypothesis: true mean is not equal to 8 90 percent confidence interval: 7.516975 8.265633 sample estimates: mean of x 7.891304 The p-value $\\approx0.623>0.1=\\alpha$. Hence, we fail to reject the null hypothesis that the population mean of sleeping times is 8 hours. In addition, we can give a 90% confidence interval for the population mean: (7.51,8.27). Note that, we round down the lower end value and round up the upper end value to make sure to contain 90% of the samples near the population mean. Nonparametric methods Nonparametric tests don't assume anything about the underlying distribution unlike the t-test we ran above. A popular one-sample nonparametric test is the Wilcoxon signed rank method for the median (not the mean). Our $\\textrm{H}_0$ is the median $M=8$. We will have a two-sided alternative and obtain a 90% interval estimate for the median of sleeping times using the following command: W = wilcox.test(sleep, mu=8, conf.int=TRUE, conf.level=0.90) Warning message in wilcox.test.default(sleep, mu = 8, conf.int = TRUE, conf.level = 0.9): \u201ccannot compute exact p-value with ties\u201d Warning message in wilcox.test.default(sleep, mu = 8, conf.int = TRUE, conf.level = 0.9): \u201ccannot compute exact confidence interval with ties\u201d Warning message in wilcox.test.default(sleep, mu = 8, conf.int = TRUE, conf.level = 0.9): \u201ccannot compute exact p-value with zeroes\u201d Warning message in wilcox.test.default(sleep, mu = 8, conf.int = TRUE, conf.level = 0.9): \u201ccannot compute exact confidence interval with zeroes\u201d Since this test uses rankings, if the data set has equal values multiple times, the command gives a warning. But it's fine, a warning message is not the same as an error message. W Wilcoxon signed rank test with continuity correction data: sleep V = 73.5, p-value = 0.3969 alternative hypothesis: true location is not equal to 8 90 percent confidence interval: 7.124979 8.374997 sample estimates: (pseudo)median 7.749961 If you're interested in learning more about how this test is run, you can watch this nice example: https://www.youtube.com/watch?v=TqCg2tb4wJ0 This test also indicates that there is insufficient evidence that the \"average\" (in the latter case, we tested the median!) sleeping time from the population is not 8 hours. We fail to reject the null hypothesis. Two Sample Inference This time, we're comparing means of two different samples. We will use the twin data set from Ashenfelter and Krueger (for the details, check the textbook and look at What Do Twins Studies Reveal About the Economic Returns to Education? twins = read.table(\"https://raw.githubusercontent.com/mariarizzo/RbyExample/master/Rx-data/twins.txt\", header=TRUE, sep=',') head(twins) A data.frame: 6 \u00d7 16 DLHRWAGE DEDUC1 AGE AGESQ HRWAGEH WHITEH MALEH EDUCH HRWAGEL WHITEL MALEL EDUCL DEDUC2 DTEN DMARRIED DUNCOV <chr> <int> <dbl> <dbl> <chr> <int> <int> <int> <chr> <int> <int> <int> <int> <chr> <int> <int> 1 0.2593466 0 33.25120 1105.6422 11.25 1 0 16 8.68 1 0 16 0 1.333 0 0 2 . -1 54.05339 2921.7688 . 1 0 9 7.85 1 0 10 1 8 1 0 3 0.721318058 7 43.57016 1898.3586 18 1 0 19 8.75 1 0 12 4 3 -1 0 4 0.011581964 0 30.96783 959.0065 16.5 1 1 12 16.31 1 1 12 0 -2 0 1 5 -0.560984677 0 34.63381 1199.5010 9.6154 1 1 14 16.85 1 1 14 1 2.917 0 -1 6 . 2 71.60301 5126.9913 . 1 0 16 . 1 0 14 -2 24 1 0 The variable HRWAGEH is the hourly wage for twin 2. twins$HRWAGEH = as.numeric(twins$HRWAGEH) Warning message in eval(expr, envir, enclos): \u201cNAs introduced by coercion\u201d hist(twins$HRWAGEH) Note that this data is strongly right-skewed. log.wages = log(twins$HRWAGEH) hist(log.wages) By taking the logarithm, we removed the skew. There are two kinds of two sample tests: 1. When you have two independent samples. 2. When you have two measurements for the same individuals (paired test, think of it as before/after). We begin with the first kind. Two sample t-test We first create our two samples as two different subsets of the participants of the twins dataset. twins$EDUCH = as.numeric(twins$EDUCH) college = ifelse(twins$EDUCH > 12, \"yes\", \"no\") table(college) college no yes 71 112 Sample 1: The ones who didn't go to college (high school). Sample 2: The ones who went to college. We will compare the average wages (to be more precise: the log wages) of those two groups. boxplot(log.wages ~ college, horizontal=TRUE, names=c(\"High School\", \"Some College\"), xlab=\"log Wage\") Let $\\mu_H$ and $\\mu_C$ denote respectively the mean log wage of the population of high school and college graduates. Null hypothesis $\\textrm{H} 0$: $\\mu_H = \\mu_H$ Alternative hypothesis $\\textrm{H} \\textrm{alt}$: $\\mu_H \\neq \\mu_H$ $\\alpha=0.05$ Actually, there are two tests which work very similar: The Welch test and the traditional t-test. The Welch test doesn't assume the variances of the two populations are different. The traditional t-test assumes the variances of the two populations are equal. The default implementation of the command t.test() in R is the Welch test. # remember, default argument for alternative is two-sided # default value for alpha is 0.05 t.test(log.wages ~ college) Welch Two Sample t-test data: log.wages by college t = -2.4545, df = 131.24, p-value = 0.01542 alternative hypothesis: true difference in means between group no and group yes is not equal to 0 95 percent confidence interval: -0.42999633 -0.04620214 sample estimates: mean in group no mean in group yes 2.282119 2.520218 The (two sided) p-value is 0.015 which is much smaller than $\\alpha=0.05$. Hence we can reject the null hypothesis that the college graduates and high school graduates have the same average (log) wages. In a scenario, where it makes sense to assume the variances of the two populations are equal, you can also use the traditional t-test by adding the argument var.equal=TRUE (default value is FALSE). t.test(log.wages ~ college, var.equal=TRUE) Two Sample t-test data: log.wages by college t = -2.3683, df = 159, p-value = 0.01907 alternative hypothesis: true difference in means between group no and group yes is not equal to 0 95 percent confidence interval: -0.43665519 -0.03954328 sample estimates: mean in group no mean in group yes 2.282119 2.520218 Two sample Mann-Whitney-Wilcoxon test This is a nonparametric alternative to the two-sample test we saw above. wilcox.test(log.wages ~ college, conf.int=TRUE) Wilcoxon rank sum test with continuity correction data: log.wages by college W = 2264, p-value = 0.01093 alternative hypothesis: true location shift is not equal to 0 95 percent confidence interval: -0.44266384 -0.06455011 sample estimates: difference in location -0.2575775 W denotes the number of pairs $(x_i,y_j)$ where $x_i>y_j$, for $x$ and $y$ coming from sample 1 and sample 2, respectively. Note that the results of both tests are similar. Permutation test Please read from the textbook. It's a fun test. Paired Sample Inference Using a t Statistic This time we will pair the indiviuals in the samples. In other words, for every individual in sample 1, there will be exactly one individual in sample 2. This test usually is applied to test the effect of a drug or an educational method to compare the before and after measurements of the same people. Here, we will be matching up twins. In the above example, we saw that the education level seem to be related to the wages. However, maybe it's not the education level alone, but there are some confounding variables which effect both the education level and the wages. To rule cancel out the effect of possible confounding variables, this time we will pick the twins for which the education levels of twin 1 ( EDUCL ) and twin 2 ( EDUCH ) are different: twins$EDUCL = as.numeric(twins$EDUCL) twins$HRWAGEL = as.numeric(twins$HRWAGEL) twins$HRWAGEH = as.numeric(twins$HRWAGEH) Warning message in eval(expr, envir, enclos): \u201cNAs introduced by coercion\u201d twins.diff = subset(twins, EDUCH != EDUCL) # number of rows nrow(twins.diff) 94 There are 94 such twins. any(is.na(twins.diff)) TRUE There seem to be some NA values in our dataframe. Digression: Finding the rows in which NA values exist. na_cells = which(is.na(twins.diff), arr.ind=TRUE) # list the cells whose values are na na_cells A matrix: 23 \u00d7 2 of type int row col 2 1 5 6 3 5 13 8 5 16 11 5 79 40 5 101 50 5 113 56 5 137 70 5 150 78 5 163 84 5 6 3 9 8 5 9 16 11 9 18 12 9 25 17 9 76 38 9 79 40 9 91 44 9 113 56 9 119 61 9 150 78 9 160 82 9 166 86 9 # list the rows of the dataframe that contain NA cells na_rows = as.numeric(rownames(na_cells)) twins[na_rows,] A data.frame: 23 \u00d7 16 DLHRWAGE DEDUC1 AGE AGESQ HRWAGEH WHITEH MALEH EDUCH HRWAGEL WHITEL MALEL EDUCL DEDUC2 DTEN DMARRIED DUNCOV <chr> <int> <dbl> <dbl> <dbl> <int> <int> <dbl> <dbl> <int> <int> <dbl> <int> <chr> <int> <int> 2 . -1 54.05339 2921.7688 NA 1 0 9 7.850000 1 0 10 1 8 1 0 6 . 2 71.60301 5126.9913 NA 1 0 16 NA 1 0 14 -2 24 1 0 13 . 4 32.08214 1029.2634 NA 1 0 16 31.250000 1 0 12 4 -9.5 0 0 16 . 4 36.04107 1298.9586 NA 1 0 18 NA 1 0 14 4 . -1 0 79 . 2 40.86242 1669.7376 NA 1 0 16 NA 1 0 14 -1 -0.5 0 0 101 . -4 62.57906 3916.1382 NA 0 1 12 12.000000 0 1 16 0 -7.75 0 0 113 . 1 78.64203 6184.5683 NA 1 1 13 NA 1 1 12 -1 15 1 0 137 . 5 27.94798 781.0896 NA 1 0 18 5.395161 1 0 13 3 0.334 0 0 150 . -1 64.45996 4155.0863 NA 0 0 11 NA 0 0 12 1 -21 1 1 163 . -1 70.60096 4984.4953 NA 1 1 12 6.868095 1 1 13 0 13 0 -1 6.1 . 2 71.60301 5126.9913 NA 1 0 16 NA 1 0 14 -2 24 1 0 8 . -1 61.45106 3776.2329 35.000000 1 0 13 NA 1 0 14 -2 25.5 0 0 16.1 . 4 36.04107 1298.9586 NA 1 0 18 NA 1 0 14 4 . -1 0 18 . 1 24.54483 602.4488 5.250000 1 1 13 NA 1 1 12 0 . 1 0 25 . 1 28.90075 835.2535 8.583333 1 1 15 NA 1 1 14 -1 0 0 0 76 . 1 27.13484 736.2995 23.750000 1 0 20 NA 1 0 19 3 3 0 1 79.1 . 2 40.86242 1669.7376 NA 1 0 16 NA 1 0 14 -1 -0.5 0 0 91 . -2 55.91239 3126.1952 20.000000 1 1 12 NA 1 1 14 -1 29 0 1 113.1 . 1 78.64203 6184.5683 NA 1 1 13 NA 1 1 12 -1 15 1 0 119 . 1 28.11499 790.4526 70.000000 1 1 16 NA 1 1 15 2 1 0 0 150.1 . -1 64.45996 4155.0863 NA 0 0 11 NA 0 0 12 1 -21 1 1 160 . 1 33.25120 1105.6422 9.600000 1 0 13 NA 1 0 12 0 -2 0 0 166 . 1 52.98563 2807.4766 14.285714 1 1 13 NA 1 1 12 0 3 0 0 # the digression above is not important in our case, # as we will simply drop the rows that contain a na cell. twins.diff = twins.diff[complete.cases(twins.diff), ] nrow(twins.diff) 76 18 rows are dropped. Now, we can run the paired t test. # for each row, determine the lower and the higher wages. log.wages.low = with(twins.diff, ifelse(EDUCL < EDUCH, log(HRWAGEL), log(HRWAGEH))) log.wages.high = with(twins.diff, ifelse(EDUCL < EDUCH, log(HRWAGEH), log(HRWAGEL))) # combine those values for each row, and obtain a dataframe low_high = cbind(log.wages.low, log.wages.high) head(low_high) A matrix: 6 \u00d7 2 of type dbl log.wages.low log.wages.high 2.169054 2.890372 3.555348 2.032088 2.484907 2.708050 2.847812 2.796061 2.748872 3.218876 1.972691 2.278292 Instead of working with two samples, we will create a new variable: difference between the low and high (log) wages. Instead of asking if $\\mu_L = \\mu_H$, we will ask if their difference $d=\\mu_H - \\mu_L$ is zero. diff = low_high[,2]-low_high[,1] hist(diff) Now, we can run the t-test for the paired samples. Summary If you can pair the samples when comparing, then group corresponding individuals from both samples and think of those groups as individuals. And run a one sample t-test for the difference of the values of those individuals. In our example, instead of comparing the wages of twin 1's and the wages of twin 2's directly, we're looking at the difference of wages of the twins (if their education level is different). Think of the new sample consisting twins (both twins together), and the random variable as the difference between their wages. # just add the argument paired=TRUE t.test(log.wages.low, log.wages.high, paired=TRUE) Paired t-test data: log.wages.low and log.wages.high t = -4.6197, df = 75, p-value = 1.563e-05 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -0.3918743 -0.1557355 sample estimates: mean of the differences -0.2738049 p-value is extremely small, hence the null hypothesis $\\mu_H = \\mu_L$ is to be rejected.","title":"06 - Chapter 6"},{"location":"chapter_6/basic_inference/#chapter-6-basic-inference-methods","text":"","title":"Chapter 6: Basic Inference Methods"},{"location":"chapter_6/basic_inference/#learning-about-a-proportion","text":"A proportion in statistics is a fraction of the population ( population proportion ) or sample ( sample proportion ) that has a certain characteristic. When we say the proportion of iPhones in the mobile phone market is 20%, then that means 20% of the mobile phones are iPhones and equivalently, we can also say that if you pick a random mobile phone, there is 20% probability that it is an iPhone. Similarly, if there are 20 students in your classroom and 2 of them are failing your class, you can say that the sample proportion of passing the class is 90%.","title":"Learning About a Proportion"},{"location":"chapter_6/basic_inference/#example-1-sleeping-patterns-of-college-students","text":"Question: Is the median hours of sleep for the population of all students who take a particular class 9 hours? Sample data: Sleeping times of 24 students who take that class. Sleeping times of students 7.75, 8.5, 8, 6, 8, 6.33, 8.17, 7.75, 7, 6.5, 8.75, 8, 7.5, 3, 6.25, 8.5, 9, 6.5, 9, 9.5, 9, 8, 8, 9.5 We will do two smart things: 1. Instead of asking if the median is 9 hours, we can equivalently ask if the proportion of students who sleep more than 9 hours is 50%. 2. We will formulate our question in form of a hypothesis test.","title":"Example 1:  Sleeping patterns of college students"},{"location":"chapter_6/basic_inference/#hypothesis-tests","text":"A hypothesis is a claim about the value of a population characteristic. A hypothesis test is like a courtroom scene: Courtroom Hypothesis Test Initial assumption: The defendent is innocent. (It's the duty of the prosecutor to prove the defendent is guilty.) The null hypothesis. (We assume some value for the population characteristic.) The prosecutor tries to prove the defendant is guilty. Alternative hypothesis. (We want to know if our sample statistic provides sufficiently strong evidence in some desired direction away from the null hypothesis value.) Evidence. Sample statistic(s). Strong evidence. Sample statistic is far (in the desired direction.) Decision: Either the defendent is guilty or is not guilty. (There is no such decision stating the defendant is innocent.) Conclusion of the test: Either the null hypothesis is rejected or the null hypothesis is failed to be rejected. One important remark is that in a court of law, the defendant is never found innocent. The prosecutor tries to prove the defendant is guilty by bringing some evidence. That evidence is discussed. If the evidence is weak, the defendant is found not guilty. If the evidence is strong, the defendant is found guilty. Similarly, in a hypothesis test, we check if the sample statistic provides strong enough evidence for the alternate hypothesis. If the evidence is weak (sample statistic not far from the null hypothesis value), we fail to reject the null hypothesis . If the evidence is strong (sample statistic is far from the null hypothesis value in the desired direction), we reject the null hypothesis . There are two kind of errors that can happen. Courtroom Hypothesis test Term An innocent person is sentenced. We reject a true null hypothesis. Type I error A guilty person is not sentenced. We fail to reject a false null hypothesis. Type II error Back to our example. sleep = c(7.75, 8.5, 8, 6, 8, 6.33, 8.17, 7.75, 7, 6.5, 8.75, 8, 7.5, 3, 6.25, 8.5, 9, 6.5, 9, 9.5, 9, 8, 8, 9.5) nine.hours = ifelse(sleep >= 9, \"yes\", \"no\") table(nine.hours) nine.hours no yes 19 5 Our sample size is 24. $$n=24$$ The null hypothesis is that the population proportion is 50%. $$\\textrm{$H_0$: }p=0.5$$ In a hypothesis test, we're both the judge and the prosecutor. As a prosecutor, we're expected to give the details of our claim. Our claim by default is the null hypothesis is not true, but what is considered not true. There are three options we can choose from: 1. The population characteristic (in this example the population proportion) is greater than the hypothesized value (upper-tailed test): $$\\textrm{H} {\\textrm{alt}:} p > p_0$$ where $p_0$ denotes the value we use in null-hypothesis. 2. The population characteristic is less than the hypothesized value (lower-tailed test): $$\\textrm{H} {\\textrm{alt}:} p < p_0$$ 3. The population characteristic is different from the hypothesized value (two-tailed test): $$\\textrm{H}_{\\textrm{alt}:} p \\neq p_0$$ As a judge, we determine how extreme the sample proportion should be so that it counts as strong evidence. That threshold is quantified as $\\alpha$. Commonly used values of $\\alpha$ are 0.1, 0.05 and 0.01. We need to decide on the value of $\\alpha$ before running the test. $\\alpha=0.05$ means, the sample statistic is considered sufficiently strong evidence for the alternative hypothesis if the sample Since we want to know if the median hours of sleep is different from 9 hours, we will say the our alternative hypothesis is that the population proportion is greater or less than 50%: $$\\textrm{$H_\\textrm{alt}$: }p\\neq0.5$$ y = 5; n = 24 Test = prop.test(y, n, p=0.5, alternative=\"two.sided\", correct=FALSE) Test 1-sample proportions test without continuity correction data: y out of n, null probability 0.5 X-squared = 8.1667, df = 1, p-value = 0.004267 alternative hypothesis: true p is not equal to 0.5 95 percent confidence interval: 0.09244825 0.40470453 sample estimates: p 0.2083333 Based on the $P$-value we obtain (which measures how far our sample proportion is from the hypothesized population proportion), we reject the nullhypothesis. Test$p.value 0.00426672482217614 Note that our $P$ value $\\approx 0.0043$ is (much) less than $\\alpha=5\\%$. That means our sample proportion is quite extreme according to the null hypothesis. Our conclusion will be that the null hypothesis seems not to be true and we reject the null hypothesis. Based on our sample proportion $\\frac{5}{24}=\\approx 20.8\\%$, the population proportion should be in the following confidence interval with a confidence level of $95\\%$: Test$conf.int .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 0.0924482535285185 0.404704532531887 In other words, if we assume that 95% of all samples are to be considered not extreme, and our sample is one of those not extreme examples, the population proportion should be in the interval $(0.092,0.405)$. Our sample is a bit problematic: Our sample size is not that big (we don't know if the sample proportions are distributed normally). So, we can't fully rely on the assumption that $\\hat{p}$'s are distributed normally. Which is why our textbook offers some other tests (which we will not discuss in detail, just know that they exist and know where to find them and how to use them), and we will pick the most conservative result.","title":"Hypothesis Tests:"},{"location":"chapter_6/basic_inference/#same-test-but-activating-the-correction-argument-of-proptest-wilson","text":"Test.adj = prop.test(y,n,p=0.5, alternative=\"two.sided\", correct=TRUE) Test.adj$p.value 0.00796348920654999 Test.adj$conf.int .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 0.0793597153947196 0.427065000253439","title":"Same test, but activating the correction argument of prop.test (Wilson)"},{"location":"chapter_6/basic_inference/#using-theoretical-probability-clopper-pearson","text":"The areas of the bars in a binomial distribution can be computed using probability. In this approach, we assume our data set approximately follows a binomial distribution. We mentioned binomial distributions last week when talking about the central limit theorem. You can read more on those distributions on this page: https://r-coder.com/binomial-distribution-r/ . Test.exact = binom.test(y,n,p=0.5) Test.exact$p.value 0.00661075115203857 Test.exact$conf.int .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 0.0713186171980549 0.421512843637253","title":"Using theoretical probability (Clopper-Pearson)"},{"location":"chapter_6/basic_inference/#using-agresti-coull-test","text":"This is another popular \"small-sample\" method. We simply modify our dataset by artificially adding two successes and two failures. R doesn't have an implemented Agresti-Coull test, but we can write it as a function ourselves quite easily: agresti.interval = function(y, n, conf=0.95){ n1 = n + 4 y1 = y + 2 phat = y1 / n1 me = qnorm(1 - (1 - conf) / 2) * sqrt(phat * (1 - phat) / n1) c(phat - me, phat + me) } agresti.interval(y, n) .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 0.0896127978831776 0.410387202116822 Now that we have computed 95% confidence intervals in three different ways, we can pick the most conservative one: cnames = c(\"Wilson Score Interval\", \"Clopper-Pearson\",\"Agresti-Coull\") cfunctions = c(\"prop.test\", \"binom.test\", \"agresti.interval\") intervals = rbind(Test$conf.int, Test.exact$conf.int,agresti.interval(y, n)) data.frame(Name=cnames, Function=cfunctions,LO=intervals[ , 1], HI=intervals[ , 2]) A data.frame: 3 \u00d7 4 Name Function LO HI <chr> <chr> <dbl> <dbl> Wilson Score Interval prop.test 0.09244825 0.4047045 Clopper-Pearson binom.test 0.07131862 0.4215128 Agresti-Coull agresti.interval 0.08961280 0.4103872 The most conservative confidence interval is the longest one (in this example, the Clopper-Pearson test). If the interval lengths are very different, more thought is to be given to which interval to choose. In our example, they are quite similar. What's really important is that we go far enough to guarantee that 95% of the sample means is covered about the population mean (at most half the length of our interval away).","title":"Using Agresti-Coull test"},{"location":"chapter_6/basic_inference/#learning-about-a-mean","text":"In the previous discussion, we did two things: 1. We tested if our sample proportion $\\hat{p}$ is far from a hypothesized population proportion (hypothesis test for population proportion). 2. Where would be the population proportion based on our sample proportion (confidence interval).","title":"Learning About a Mean"},{"location":"chapter_6/basic_inference/#one-sample-t-statistic-methods","text":"We can play the same game for the sample mean $\\bar{x}$. Remember that the sample means are also distributed normally with mean $\\mu_{\\bar{x}}$ equal to the population mean $\\mu_x$. $$\\textrm{$H_0$: }\\mu=8$$ $$n=24$$ $$\\alpha=0.1$$ Strictly speaking, we shouldn't really assume that the mean sleeping times are normally distributed, hence that $n$ is not large enough (>30). (Note that if the population data itself is normally distributed, then $n$ doesn't need to be large, the sample means would be distributed normally even when the sample size is small.) How do we check if our sample data comes from a data set that is normally distributed? One way is doing by hand: To check if our data is normally distributed, we can look at the histogram of sample data, and we can compare the quartiles of a normal distribution with the quartiles of our sample data. A sample that is chosen from a normal distribution is expected to be more or less normally distributed itself. sleep .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 7.75 8.5 8 6 8 6.33 8.17 7.75 7 6.5 8.75 8 7.5 3 6.25 8.5 9 6.5 9 9.5 9 8 8 9.5 hist(sleep,prob=TRUE) One visual way to check if the sample data seems to come from a normal distribution is checking its histogram. The histogram in this example has a problematic bar (rightmost). A more grounded way of checking if our sample data is close to a normal distribution is to break the area under the normal curve into 24 equal regions bounded by the x-axis below, normal distribution above and two consecutive horizontal cuts. Then we compare those cuts with our sample data. If those cut values are similar to our sample data, then our 24 values seem to be distributed more or less normally and we can conclude $n$ doesn't need to be large to apply the t-test. # cuts of normal distribution ppoints(sleep) .list-inline {list-style: none; margin:0; padding: 0} .list-inline>li {display: inline-block} .list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex} 0.0208333333333333 0.0625 0.104166666666667 0.145833333333333 0.1875 0.229166666666667 0.270833333333333 0.3125 0.354166666666667 0.395833333333333 0.4375 0.479166666666667 0.520833333333333 0.5625 0.604166666666667 0.645833333333333 0.6875 0.729166666666667 0.770833333333333 0.8125 0.854166666666667 0.895833333333333 0.9375 0.979166666666667 qqnorm(sleep) # produces a normal probability plot of the times qqline(sleep) # overlays a line on the plot passing through the first and third quartiles For more about Q-Q plots, you can read: https://data.library.virginia.edu/understanding-q-q-plots/ Note that the point in the bottom left corner is super weird. We can detect the location of that outlier from the raw data or visually using the plot() function: plot(sleep) The sleep time of the outlier seems to be 3. So, let's find the index of that value in the sleep vector: min(sleep) 3 That value is 3 indeed. Now, let's locate the position of the value 3 in the sleep vector: which(sleep<3.5) 14 # double checking if the 14th term of the sleep vector is 3. sleep[14] 3 One way to deal with that outlier is to simply ignore it. # -14 inside the square brackets drops the term in the 14th position sleep.new = sleep[-14]","title":"One-sample t statistic methods"},{"location":"chapter_6/basic_inference/#digression-how-to-remove-values-from-a-vector","text":"https://www.statology.org/remove-element-from-vector-r/","title":"Digression: How to remove values from a vector:"},{"location":"chapter_6/basic_inference/#digression-shapiro-wilk-test","text":"There is actually a statistical test that measures normality of the population distribution based on the sample data. shapiro.test(sleep) Shapiro-Wilk normality test data: sleep W = 0.87238, p-value = 0.005858 The p-value is much less than $\\alpha=0.05$, so we cannot assume that the sample data comes from a normally distributed population. And for that reason, t-test may not be accurate. To read more about the Shapiro-Wilk test: https://www.statology.org/shapiro-wilk-test-r/ Now that the outlier is removed, we can run a t-test : t.test(sleep.new, mu=8, conf.level=0.90) One Sample t-test data: sleep.new t = -0.49862, df = 22, p-value = 0.623 alternative hypothesis: true mean is not equal to 8 90 percent confidence interval: 7.516975 8.265633 sample estimates: mean of x 7.891304 The p-value $\\approx0.623>0.1=\\alpha$. Hence, we fail to reject the null hypothesis that the population mean of sleeping times is 8 hours. In addition, we can give a 90% confidence interval for the population mean: (7.51,8.27). Note that, we round down the lower end value and round up the upper end value to make sure to contain 90% of the samples near the population mean.","title":"Digression: Shapiro-Wilk Test"},{"location":"chapter_6/basic_inference/#nonparametric-methods","text":"Nonparametric tests don't assume anything about the underlying distribution unlike the t-test we ran above. A popular one-sample nonparametric test is the Wilcoxon signed rank method for the median (not the mean). Our $\\textrm{H}_0$ is the median $M=8$. We will have a two-sided alternative and obtain a 90% interval estimate for the median of sleeping times using the following command: W = wilcox.test(sleep, mu=8, conf.int=TRUE, conf.level=0.90) Warning message in wilcox.test.default(sleep, mu = 8, conf.int = TRUE, conf.level = 0.9): \u201ccannot compute exact p-value with ties\u201d Warning message in wilcox.test.default(sleep, mu = 8, conf.int = TRUE, conf.level = 0.9): \u201ccannot compute exact confidence interval with ties\u201d Warning message in wilcox.test.default(sleep, mu = 8, conf.int = TRUE, conf.level = 0.9): \u201ccannot compute exact p-value with zeroes\u201d Warning message in wilcox.test.default(sleep, mu = 8, conf.int = TRUE, conf.level = 0.9): \u201ccannot compute exact confidence interval with zeroes\u201d Since this test uses rankings, if the data set has equal values multiple times, the command gives a warning. But it's fine, a warning message is not the same as an error message. W Wilcoxon signed rank test with continuity correction data: sleep V = 73.5, p-value = 0.3969 alternative hypothesis: true location is not equal to 8 90 percent confidence interval: 7.124979 8.374997 sample estimates: (pseudo)median 7.749961 If you're interested in learning more about how this test is run, you can watch this nice example: https://www.youtube.com/watch?v=TqCg2tb4wJ0 This test also indicates that there is insufficient evidence that the \"average\" (in the latter case, we tested the median!) sleeping time from the population is not 8 hours. We fail to reject the null hypothesis.","title":"Nonparametric methods"},{"location":"chapter_6/basic_inference/#two-sample-inference","text":"This time, we're comparing means of two different samples. We will use the twin data set from Ashenfelter and Krueger (for the details, check the textbook and look at What Do Twins Studies Reveal About the Economic Returns to Education? twins = read.table(\"https://raw.githubusercontent.com/mariarizzo/RbyExample/master/Rx-data/twins.txt\", header=TRUE, sep=',') head(twins) A data.frame: 6 \u00d7 16 DLHRWAGE DEDUC1 AGE AGESQ HRWAGEH WHITEH MALEH EDUCH HRWAGEL WHITEL MALEL EDUCL DEDUC2 DTEN DMARRIED DUNCOV <chr> <int> <dbl> <dbl> <chr> <int> <int> <int> <chr> <int> <int> <int> <int> <chr> <int> <int> 1 0.2593466 0 33.25120 1105.6422 11.25 1 0 16 8.68 1 0 16 0 1.333 0 0 2 . -1 54.05339 2921.7688 . 1 0 9 7.85 1 0 10 1 8 1 0 3 0.721318058 7 43.57016 1898.3586 18 1 0 19 8.75 1 0 12 4 3 -1 0 4 0.011581964 0 30.96783 959.0065 16.5 1 1 12 16.31 1 1 12 0 -2 0 1 5 -0.560984677 0 34.63381 1199.5010 9.6154 1 1 14 16.85 1 1 14 1 2.917 0 -1 6 . 2 71.60301 5126.9913 . 1 0 16 . 1 0 14 -2 24 1 0 The variable HRWAGEH is the hourly wage for twin 2. twins$HRWAGEH = as.numeric(twins$HRWAGEH) Warning message in eval(expr, envir, enclos): \u201cNAs introduced by coercion\u201d hist(twins$HRWAGEH) Note that this data is strongly right-skewed. log.wages = log(twins$HRWAGEH) hist(log.wages) By taking the logarithm, we removed the skew. There are two kinds of two sample tests: 1. When you have two independent samples. 2. When you have two measurements for the same individuals (paired test, think of it as before/after). We begin with the first kind.","title":"Two Sample Inference"},{"location":"chapter_6/basic_inference/#two-sample-t-test","text":"We first create our two samples as two different subsets of the participants of the twins dataset. twins$EDUCH = as.numeric(twins$EDUCH) college = ifelse(twins$EDUCH > 12, \"yes\", \"no\") table(college) college no yes 71 112 Sample 1: The ones who didn't go to college (high school). Sample 2: The ones who went to college. We will compare the average wages (to be more precise: the log wages) of those two groups. boxplot(log.wages ~ college, horizontal=TRUE, names=c(\"High School\", \"Some College\"), xlab=\"log Wage\") Let $\\mu_H$ and $\\mu_C$ denote respectively the mean log wage of the population of high school and college graduates. Null hypothesis $\\textrm{H} 0$: $\\mu_H = \\mu_H$ Alternative hypothesis $\\textrm{H} \\textrm{alt}$: $\\mu_H \\neq \\mu_H$ $\\alpha=0.05$ Actually, there are two tests which work very similar: The Welch test and the traditional t-test. The Welch test doesn't assume the variances of the two populations are different. The traditional t-test assumes the variances of the two populations are equal. The default implementation of the command t.test() in R is the Welch test. # remember, default argument for alternative is two-sided # default value for alpha is 0.05 t.test(log.wages ~ college) Welch Two Sample t-test data: log.wages by college t = -2.4545, df = 131.24, p-value = 0.01542 alternative hypothesis: true difference in means between group no and group yes is not equal to 0 95 percent confidence interval: -0.42999633 -0.04620214 sample estimates: mean in group no mean in group yes 2.282119 2.520218 The (two sided) p-value is 0.015 which is much smaller than $\\alpha=0.05$. Hence we can reject the null hypothesis that the college graduates and high school graduates have the same average (log) wages. In a scenario, where it makes sense to assume the variances of the two populations are equal, you can also use the traditional t-test by adding the argument var.equal=TRUE (default value is FALSE). t.test(log.wages ~ college, var.equal=TRUE) Two Sample t-test data: log.wages by college t = -2.3683, df = 159, p-value = 0.01907 alternative hypothesis: true difference in means between group no and group yes is not equal to 0 95 percent confidence interval: -0.43665519 -0.03954328 sample estimates: mean in group no mean in group yes 2.282119 2.520218","title":"Two sample t-test"},{"location":"chapter_6/basic_inference/#two-sample-mann-whitney-wilcoxon-test","text":"This is a nonparametric alternative to the two-sample test we saw above. wilcox.test(log.wages ~ college, conf.int=TRUE) Wilcoxon rank sum test with continuity correction data: log.wages by college W = 2264, p-value = 0.01093 alternative hypothesis: true location shift is not equal to 0 95 percent confidence interval: -0.44266384 -0.06455011 sample estimates: difference in location -0.2575775 W denotes the number of pairs $(x_i,y_j)$ where $x_i>y_j$, for $x$ and $y$ coming from sample 1 and sample 2, respectively. Note that the results of both tests are similar.","title":"Two sample Mann-Whitney-Wilcoxon test"},{"location":"chapter_6/basic_inference/#permutation-test","text":"Please read from the textbook. It's a fun test.","title":"Permutation test"},{"location":"chapter_6/basic_inference/#paired-sample-inference-using-a-t-statistic","text":"This time we will pair the indiviuals in the samples. In other words, for every individual in sample 1, there will be exactly one individual in sample 2. This test usually is applied to test the effect of a drug or an educational method to compare the before and after measurements of the same people. Here, we will be matching up twins. In the above example, we saw that the education level seem to be related to the wages. However, maybe it's not the education level alone, but there are some confounding variables which effect both the education level and the wages. To rule cancel out the effect of possible confounding variables, this time we will pick the twins for which the education levels of twin 1 ( EDUCL ) and twin 2 ( EDUCH ) are different: twins$EDUCL = as.numeric(twins$EDUCL) twins$HRWAGEL = as.numeric(twins$HRWAGEL) twins$HRWAGEH = as.numeric(twins$HRWAGEH) Warning message in eval(expr, envir, enclos): \u201cNAs introduced by coercion\u201d twins.diff = subset(twins, EDUCH != EDUCL) # number of rows nrow(twins.diff) 94 There are 94 such twins. any(is.na(twins.diff)) TRUE There seem to be some NA values in our dataframe.","title":"Paired Sample Inference Using a t Statistic"},{"location":"chapter_6/basic_inference/#digression-finding-the-rows-in-which-na-values-exist","text":"na_cells = which(is.na(twins.diff), arr.ind=TRUE) # list the cells whose values are na na_cells A matrix: 23 \u00d7 2 of type int row col 2 1 5 6 3 5 13 8 5 16 11 5 79 40 5 101 50 5 113 56 5 137 70 5 150 78 5 163 84 5 6 3 9 8 5 9 16 11 9 18 12 9 25 17 9 76 38 9 79 40 9 91 44 9 113 56 9 119 61 9 150 78 9 160 82 9 166 86 9 # list the rows of the dataframe that contain NA cells na_rows = as.numeric(rownames(na_cells)) twins[na_rows,] A data.frame: 23 \u00d7 16 DLHRWAGE DEDUC1 AGE AGESQ HRWAGEH WHITEH MALEH EDUCH HRWAGEL WHITEL MALEL EDUCL DEDUC2 DTEN DMARRIED DUNCOV <chr> <int> <dbl> <dbl> <dbl> <int> <int> <dbl> <dbl> <int> <int> <dbl> <int> <chr> <int> <int> 2 . -1 54.05339 2921.7688 NA 1 0 9 7.850000 1 0 10 1 8 1 0 6 . 2 71.60301 5126.9913 NA 1 0 16 NA 1 0 14 -2 24 1 0 13 . 4 32.08214 1029.2634 NA 1 0 16 31.250000 1 0 12 4 -9.5 0 0 16 . 4 36.04107 1298.9586 NA 1 0 18 NA 1 0 14 4 . -1 0 79 . 2 40.86242 1669.7376 NA 1 0 16 NA 1 0 14 -1 -0.5 0 0 101 . -4 62.57906 3916.1382 NA 0 1 12 12.000000 0 1 16 0 -7.75 0 0 113 . 1 78.64203 6184.5683 NA 1 1 13 NA 1 1 12 -1 15 1 0 137 . 5 27.94798 781.0896 NA 1 0 18 5.395161 1 0 13 3 0.334 0 0 150 . -1 64.45996 4155.0863 NA 0 0 11 NA 0 0 12 1 -21 1 1 163 . -1 70.60096 4984.4953 NA 1 1 12 6.868095 1 1 13 0 13 0 -1 6.1 . 2 71.60301 5126.9913 NA 1 0 16 NA 1 0 14 -2 24 1 0 8 . -1 61.45106 3776.2329 35.000000 1 0 13 NA 1 0 14 -2 25.5 0 0 16.1 . 4 36.04107 1298.9586 NA 1 0 18 NA 1 0 14 4 . -1 0 18 . 1 24.54483 602.4488 5.250000 1 1 13 NA 1 1 12 0 . 1 0 25 . 1 28.90075 835.2535 8.583333 1 1 15 NA 1 1 14 -1 0 0 0 76 . 1 27.13484 736.2995 23.750000 1 0 20 NA 1 0 19 3 3 0 1 79.1 . 2 40.86242 1669.7376 NA 1 0 16 NA 1 0 14 -1 -0.5 0 0 91 . -2 55.91239 3126.1952 20.000000 1 1 12 NA 1 1 14 -1 29 0 1 113.1 . 1 78.64203 6184.5683 NA 1 1 13 NA 1 1 12 -1 15 1 0 119 . 1 28.11499 790.4526 70.000000 1 1 16 NA 1 1 15 2 1 0 0 150.1 . -1 64.45996 4155.0863 NA 0 0 11 NA 0 0 12 1 -21 1 1 160 . 1 33.25120 1105.6422 9.600000 1 0 13 NA 1 0 12 0 -2 0 0 166 . 1 52.98563 2807.4766 14.285714 1 1 13 NA 1 1 12 0 3 0 0 # the digression above is not important in our case, # as we will simply drop the rows that contain a na cell. twins.diff = twins.diff[complete.cases(twins.diff), ] nrow(twins.diff) 76 18 rows are dropped. Now, we can run the paired t test. # for each row, determine the lower and the higher wages. log.wages.low = with(twins.diff, ifelse(EDUCL < EDUCH, log(HRWAGEL), log(HRWAGEH))) log.wages.high = with(twins.diff, ifelse(EDUCL < EDUCH, log(HRWAGEH), log(HRWAGEL))) # combine those values for each row, and obtain a dataframe low_high = cbind(log.wages.low, log.wages.high) head(low_high) A matrix: 6 \u00d7 2 of type dbl log.wages.low log.wages.high 2.169054 2.890372 3.555348 2.032088 2.484907 2.708050 2.847812 2.796061 2.748872 3.218876 1.972691 2.278292 Instead of working with two samples, we will create a new variable: difference between the low and high (log) wages. Instead of asking if $\\mu_L = \\mu_H$, we will ask if their difference $d=\\mu_H - \\mu_L$ is zero. diff = low_high[,2]-low_high[,1] hist(diff) Now, we can run the t-test for the paired samples. Summary If you can pair the samples when comparing, then group corresponding individuals from both samples and think of those groups as individuals. And run a one sample t-test for the difference of the values of those individuals. In our example, instead of comparing the wages of twin 1's and the wages of twin 2's directly, we're looking at the difference of wages of the twins (if their education level is different). Think of the new sample consisting twins (both twins together), and the random variable as the difference between their wages. # just add the argument paired=TRUE t.test(log.wages.low, log.wages.high, paired=TRUE) Paired t-test data: log.wages.low and log.wages.high t = -4.6197, df = 75, p-value = 1.563e-05 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -0.3918743 -0.1557355 sample estimates: mean of the differences -0.2738049 p-value is extremely small, hence the null hypothesis $\\mu_H = \\mu_L$ is to be rejected.","title":"Digression: Finding the rows in which NA values exist."}]}